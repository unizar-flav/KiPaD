{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqxCAIdNQJvplUotPxp4oe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unizar-flav/KiPaD/blob/master/KiPaD_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kinetic Parameter Determination"
      ],
      "metadata": {
        "id": "-uahnpGtYp8-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8EAFRoSbpKG",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Modules and functions\n",
        "#@markdown You need to run this cell only once regardless of the number of datasets to be evaluated.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import zipfile # Necesary to compress the files into zip\n",
        "import csv\n",
        "\n",
        "from scipy.linalg import svd\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "from bokeh.io import output_notebook, show, export_png\n",
        "from bokeh.plotting import figure, output_file, save, show\n",
        "\n",
        "from bokeh.palettes import linear_palette, Viridis256\n",
        "from bokeh.palettes import Category20\n",
        "from bokeh.models import Button, CustomJS, TabPanel, Tabs, Legend, Span, Label\n",
        "from bokeh.layouts import column\n",
        "from bokeh.transform import linear_cmap\n",
        "output_notebook()\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "!git clone https://github.com/Mario-uni/KiPaD.git\n",
        "from KiPaD.funcionesGenerales import  procesa, argLeastSquares, deriv_RK\n",
        "\n",
        "\n",
        "# Function lee_espectro\n",
        "def lee_espectro(nombrFichs, skip_rows= 0):\n",
        "  df_list = [] # Initialize an empty list to store DataFrames\n",
        "\n",
        "  for fich in nombrFichs:\n",
        "    #REead each file into a DataFrame\n",
        "    temp_df = pd.read_csv(fich, skiprows=[skip_rows], index_col = 0)\n",
        "    df_list.append(temp_df) # Append each DataFrame to the list\n",
        "\n",
        "  # Concatenate all DataFrames into one\n",
        "  df = pd.concat(df_list)\n",
        "\n",
        "  # Sort the resulting DataFrame by index\n",
        "  df = df.sort_index()\n",
        "\n",
        "  # Let's get a name for the plots to follow which data was uploaded\n",
        "  main = next((fich for fich in nombrFichs if \"_t\" in fich), None)\n",
        "  return df, main\n",
        "\n",
        "\n",
        "# Plot function\n",
        "def create_spectra_plot(df, Title, x_axis, y_axis, Legend, width=1200, height=700):\n",
        "    # Create a figure\n",
        "    p = figure(title=Title,\n",
        "               x_axis_label=x_axis,\n",
        "               y_axis_label=y_axis,\n",
        "               width=width, height=height)\n",
        "\n",
        "    # Define font sizes for the title, axes, and labels\n",
        "    p.title.text_font_size = '20pt'\n",
        "    p.xaxis.axis_label_text_font_size = '16pt'\n",
        "    p.yaxis.axis_label_text_font_size = '16pt'\n",
        "    p.xaxis.major_label_text_font_size = '12pt'\n",
        "    p.yaxis.major_label_text_font_size = '12pt'\n",
        "\n",
        "    # Generate a color palette using Viridis256\n",
        "    n_lines = len(df.columns)\n",
        "    colors = linear_palette(Viridis256, n_lines)\n",
        "\n",
        "\n",
        "    indices = pd.to_numeric(df.index)\n",
        "    # Plot each column as a line\n",
        "    for idx, col in enumerate(df.columns):\n",
        "        p.line(indices, df[col], legend_label=str(col), line_width=2, color=colors[idx])\n",
        "        #p.line(pd.to_numeric(df.index), df[col], legend_label=str(col), line_width=2, color=colors[idx])\n",
        "    # Customize the legend\n",
        "    p.legend.title = Legend\n",
        "    p.legend.location = \"top_right\"\n",
        "    p.legend.click_policy = \"hide\"  # Allows hiding lines by clicking their labels\n",
        "    p.toolbar_location = \"below\"\n",
        "    p.legend.visible = False # Initially hide the legend\n",
        "    p.legend.label_text_font_size = '12pt'\n",
        "    p.legend.title_text_font_size = '14pt'\n",
        "\n",
        "    # Create a button to toggle the legend visibility\n",
        "    button = Button(label=\"Toggle Legend\", button_type= \"success\")\n",
        "\n",
        "    # Custom JavaScript to toggle legend visibility\n",
        "    button.js_on_click(CustomJS(args=dict(legend=p.legend[0]), code= \"\"\"\n",
        "    legend.visible = !legend.visible;  // Toggle the visibility\n",
        "\"\"\"))\n",
        "\n",
        "    # Return the plot object and button as a column layout\n",
        "    return column(p,button)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Scree Plot Method, with an elbow selection criterion based on the regression coefficient\n",
        "\n",
        "\n",
        "def scree_plot_with_fit(singular_values, threshold):\n",
        "    '''\n",
        "    Plots scree plot of singular values and determine significant values using a linear fit.\n",
        "\n",
        "    Parameters:\n",
        "        singular_values (array-like): Array of singular values.\n",
        "        threshold (float): Regression coefficient threshold (between 0 and 1) for linear fit.\n",
        "\n",
        "    Returns:\n",
        "        dict: Number of significant singular values and a Bokeh plot.\n",
        "    '''\n",
        "\n",
        "    n_values = len(singular_values)\n",
        "    SSVs = 0  # Number of significant singular values to keep\n",
        "\n",
        "    # Initialize Bokeh figure\n",
        "    p = figure(title = \" Scree Plot with Linear Fit\",\n",
        "               x_axis_label=\"Singular Value Index\",\n",
        "               y_axis_label= \"Singular Values\",\n",
        "               width= 800, height=600)\n",
        "    p.title.text_font_size = '20pt'\n",
        "    p.xaxis.axis_label_text_font_size = '16pt'\n",
        "    p.yaxis.axis_label_text_font_size = '16pt'\n",
        "    p.xaxis.major_label_text_font_size = '12pt'\n",
        "    p.yaxis.major_label_text_font_size = '12pt'\n",
        "\n",
        "    # Plot singular values\n",
        "    indices = np.arange(1, n_values +1)\n",
        "    p.scatter(indices, singular_values, size=8, color='blue', legend_label= \"Singular Values\")\n",
        "\n",
        "    # Placeholder variables for the inear fit line data\n",
        "    X_final, y_final_pred = None, None\n",
        "\n",
        "\n",
        "\n",
        "    # Iterate through singular values, trying linear fits\n",
        "    for i in range(2, n_values + 1):  # Start with at least two points for linear regression\n",
        "        X = np.arange(1, i + 1).reshape(-1, 1)\n",
        "        y = singular_values[:i]\n",
        "\n",
        "        # Perform linear regression\n",
        "        model = LinearRegression().fit(X, y)\n",
        "        r_squared = model.score(X, y)  # Get the R^2 (regression coefficient)\n",
        "\n",
        "        # If the fit quality falls below the threshold, stop\n",
        "        if r_squared < threshold:\n",
        "            SSVs = i - 1\n",
        "            break\n",
        "        else:\n",
        "            SSVs = i\n",
        "        # Update the final data for the significant linear fit\n",
        "        X_final = np.arange(1, SSVs + 1).reshape(-1,1)\n",
        "        y_final_pred = model.predict(X_final)\n",
        "\n",
        "    # Plot the linear fit up to the last significant singular value\n",
        "    p.line(X_final.flatten(), y_final_pred, line_width = 2, color= \"red\", line_dash= \"dashed\",\n",
        "                           legend_label= \"Linear Fit\")\n",
        "\n",
        "    # Add a vertical line to mark the cutoff for significant values\n",
        "    cutoff_line = Span(location= SSVs, dimension= 'height', line_color=\"green\", line_dash=\"dashed\")\n",
        "    p.add_layout(cutoff_line)\n",
        "\n",
        "    # Add a label indicating the cutoff\n",
        "    cutoff_label = Label(x=SSVs, y=singular_values[SSVs - 1], text = f'Significant Count = {SSVs}',\n",
        "                         text_color='green', y_offset=10)\n",
        "    p.add_layout(cutoff_label)\n",
        "\n",
        "    # Customize Legend and toolbar\n",
        "    p.legend.title = \"Legend\"\n",
        "    p.legend.location = \"top_right\"\n",
        "    p.legend.click_policy = \"hide\"\n",
        "    p.toolbar_location = \"below\"\n",
        "\n",
        "    # Add a toogle button to control the legend visibility\n",
        "    button = Button(label = \"Toggle Legend\", button_type = \"success\")\n",
        "    button.js_on_click(CustomJS(args=dict(legend=p.legend[0]),\n",
        "                                code=\"\"\"\n",
        "                                legend.visible = ! legend.visible;\n",
        "                                \"\"\"))\n",
        "\n",
        "    #Show the plot with the toggle button\n",
        "    plot=column(p,button)\n",
        "    sol={'SSVs':SSVs, \"plot\":plot}\n",
        "    return sol\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def entropy_selection(singular_values, entropy_threshold):\n",
        "\n",
        "    '''\n",
        "    Entropy based method to determine the number of significant singular values.\n",
        "\n",
        "    Parameters:\n",
        "        singular_values (array-like): Array of singular values.\n",
        "        threshold (float): PENDING\n",
        "    Returns:\n",
        "        dict: Number of significant singular values and a Bokeh plot.\n",
        "    '''\n",
        "    total_energy = np.sum(singular_values ** 2)\n",
        "\n",
        "    # Calculate normalized singular values (f_j)\n",
        "    f_j = singular_values ** 2 / total_energy\n",
        "\n",
        "    # Calculate entropy\n",
        "    entropy_val = np.sum(f_j * np.log(f_j)) / np.log(len(singular_values))\n",
        "\n",
        "    # Uncomment the line below in order to check the entropy of the singular values:\n",
        "    #print(f\"\\t Entropy of singular values: {entropy_val:.4f}\")\n",
        "\n",
        "    # Calculate cumulative entropy for each k\n",
        "    cumulative_entropy = np.zeros(len(singular_values))\n",
        "    for k in range(len(singular_values)):\n",
        "        ff = f_j[:k+1]\n",
        "        cumulative_entropy[k]=np.sum(ff*np.log(ff)/np.log(len(singular_values)))\n",
        "    percentage=cumulative_entropy/entropy_val\n",
        "    #print(percentage)\n",
        "\n",
        "    # Find the smallest index k such that cumulative entropy meets the threshold\n",
        "    significant_indices = np.where(percentage >= entropy_threshold)[0]\n",
        "\n",
        "    if len(significant_indices) == 0:\n",
        "        return 0  # Return 0 if no significant indices found\n",
        "    else:\n",
        "        return significant_indices[0] + 1  # Return the number of significant components\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Broken Stick Method\n",
        "def broken_stick_method(singular_values):\n",
        "    '''\n",
        "    Broken Stick Method to determine the number of significant singular values.\n",
        "\n",
        "    Parameters:\n",
        "        singular_values (array-like): Array of singular values.\n",
        "\n",
        "    Returns:\n",
        "        dict: Number of significant singular values and a Bokeh plot.\n",
        "    '''\n",
        "    k = len(singular_values)\n",
        "\n",
        "    # Calculate the broken stick values\n",
        "    broken_stick = np.zeros(k)\n",
        "    for i in range(1, k + 1):\n",
        "        broken_stick[i - 1] = (1 / k) * np.sum([1 / j for j in range(i, k + 1)])\n",
        "\n",
        "    # Normalize the singular values and broken stick values for comparison\n",
        "    singular_values_normalized = singular_values / np.sum(singular_values)\n",
        "    broken_stick_normalized = broken_stick / np.sum(broken_stick)\n",
        "\n",
        "    # Initialize Bokeh figure\n",
        "    p = figure(title=\"Broken Stick Model vs Singular Values\",\n",
        "               x_axis_label=\"Index\",\n",
        "               y_axis_label=\"Proportion of Variance\",\n",
        "               width=800, height=600)\n",
        "    p.title.text_font_size = '20pt'\n",
        "    p.xaxis.axis_label_text_font_size = '16pt'\n",
        "    p.yaxis.axis_label_text_font_size = '16pt'\n",
        "    p.xaxis.major_label_text_font_size = '12pt'\n",
        "    p.yaxis.major_label_text_font_size = '12pt'\n",
        "\n",
        "    # Plot the normalized singular values\n",
        "    indices = np.arange(1, k + 1)\n",
        "    p.line(indices, singular_values_normalized, line_width=2, color=\"blue\", legend_label=\"Singular Values\")\n",
        "    p.scatter(indices, singular_values_normalized, size=8, color=\"blue\")\n",
        "\n",
        "    # Plot the normalized broken stick values\n",
        "    p.line(indices, broken_stick_normalized, line_width=2, line_dash=\"dashed\", color=\"red\", legend_label=\"Broken Stick\")\n",
        "    p.scatter(indices, broken_stick_normalized, size=8, color=\"red\")\n",
        "\n",
        "    # Customize legend and toolbar\n",
        "    p.legend.title = \"Legend\"\n",
        "    p.legend.location = \"top_right\"\n",
        "    p.legend.click_policy = \"hide\"\n",
        "    p.toolbar_location = \"below\"\n",
        "\n",
        "    # Determine the number of significant singular values using the broken stick rule\n",
        "    SSVs = np.where(singular_values_normalized > broken_stick_normalized)[0][-1] + 1\n",
        "\n",
        "    SSVs = 0\n",
        "    for i in range(k):\n",
        "      if singular_values_normalized[i]> broken_stick_normalized[i]:\n",
        "        SSVs += 1\n",
        "      else:\n",
        "        break\n",
        "\n",
        "    # Add a toggle button to control the legend visibility\n",
        "    button = Button(label=\"Toggle Legend\", button_type=\"success\")\n",
        "    button.js_on_click(CustomJS(args=dict(legend=p.legend[0]), code=\"\"\"\n",
        "        legend.visible = !legend.visible;\n",
        "    \"\"\"))\n",
        "\n",
        "    #Show the plot with the toggle button\n",
        "    plot=column(p,button)\n",
        "    sol={'SSVs':SSVs, \"plot\":plot}\n",
        "    return sol\n",
        "\n",
        "\n",
        "\n",
        "# Function for matrix approximation from the number of SSV selected:\n",
        "def matrix_approximation(A, n):\n",
        "    \"\"\"\n",
        "    Approximates matrix A using the top n singular values.\n",
        "\n",
        "    Parameters:\n",
        "    - A: The original matrix to approximate.\n",
        "    - n: Number of significant singular values to use for approximation.\n",
        "\n",
        "    Returns:\n",
        "    - A_approx: The approximated matrix.\n",
        "    \"\"\"\n",
        "    # Perform SVD using scipy.linalg.svd\n",
        "    U, Sigma, VT = svd(A, full_matrices=False)\n",
        "\n",
        "    # Truncate the matrices to keep only the top 'n' singular values\n",
        "    U_n = U[:, :n]             # Keep the first 'n' columns of U\n",
        "    Sigma_n = np.diag(Sigma[:n])  # Keep the first 'n' singular values (diagonal matrix)\n",
        "    VT_n = VT[:n, :]           # Keep the first 'n' rows of V^T\n",
        "\n",
        "    # Compute the approximated matrix\n",
        "    A_approx = np.dot(U_n, np.dot(Sigma_n, VT_n))  # A_approx = U_n * Sigma_n * VT_n\n",
        "\n",
        "    return A_approx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Functions for the model\n",
        "\n",
        "\n",
        " # Name pending\n",
        "def create_ode_matrix(n_species, k_vals):\n",
        "    \"\"\"\n",
        "    Creates the ODE matrix to represent a system of species with the specified rate constants.\n",
        "\n",
        "    Parameters:\n",
        "        n_species (int): Number of species in the system.\n",
        "        params (dict): Dictionary of rate constants, e.g., {'k1': value, 'k_1': value, 'k2': value, ...}.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Matrix that aligns with the ODEs specified for each species.\n",
        "    \"\"\"\n",
        "    # Initialize an n_species x n_species matrix with zeros\n",
        "    ode_matrix = np.zeros((n_species, n_species))\n",
        "\n",
        "    # Populate the ODE matrix according to the specified rules\n",
        "    for i in range(n_species):\n",
        "        # Rate constant for reaction from species i to species i+1, if within bounds\n",
        "        if i + 1 < n_species:\n",
        "            ode_matrix[i, i] -= k_vals.get(f'k{i+1}', 0)      # Outflow from species i to i+1\n",
        "            ode_matrix[i + 1, i] += k_vals.get(f'k{i+1}', 0)  # Inflow to species i+1 from i\n",
        "\n",
        "        # Rate constant for reaction from species i+1 back to species i, if within bounds\n",
        "        if i - 1 >= 0:\n",
        "            ode_matrix[i, i] -= k_vals.get(f'k_{i}', 0)       # Outflow from species i to i-1\n",
        "            ode_matrix[i - 1, i] += k_vals.get(f'k_{i}', 0)   # Inflow to species i-1 from i\n",
        "\n",
        "    return ode_matrix\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def deriv_conc(conc,t, ks_matrix):\n",
        "  '''\n",
        "  Calculate the concentration derivative of every species .\n",
        "  The system of ODE's characterizing the reaction model is passed as a matrix with the rate constants\n",
        "  as coefficients. (REVISE)\n",
        "\n",
        "  Parameters\n",
        "      conc: Array con las concentraciones de las especies.\n",
        "      t : times points at which to calculate de derivative of the concentration with respect time.\n",
        "      params: a dictionary that contains the rate constants.\n",
        "\n",
        "  Returns:\n",
        "      np.ndarray: vector that contains the derivative of the concentrations of each species\n",
        "                  at the specified time point t. the rate of change of conecntration for each\n",
        "                  species in the system at the given time t.\n",
        "\n",
        "  '''\n",
        "  return np.dot(ks_matrix, conc)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Algoritmo Runge-Kutta 4º orden\n",
        "def deriv_RK (fDeriv,x,t,deltaT,paramDeriv):\n",
        "  #print(f\"paramDeriv is: {paramDeriv}\")\n",
        "  k1 = fDeriv ( x, t, paramDeriv)\n",
        "  k2 = fDeriv ( x+k1*deltaT/2, t+ deltaT/2, paramDeriv)\n",
        "  k3 = fDeriv ( x+k2*deltaT/2, t+ deltaT/2, paramDeriv)\n",
        "  k4 = fDeriv ( x+k3*deltaT, t+ deltaT, paramDeriv)\n",
        "\n",
        "  return (k1+2*k2+2*k3+k4)/6\n",
        "\n",
        "def solv_conc_profile (k_vals, f_deriv, Conc_0, t):\n",
        "  \"\"\"\n",
        "  Solves the concentration profile of the reaction over time using\n",
        "  a 4th-order Runge-Kutta (RK4) method, allowing for variable time steps.\n",
        "\n",
        "  Parameters:\n",
        "  - f: Function that computes the derivative (reaction model)\n",
        "  - y0: Initial concentrations of the species\n",
        "  - t: Array or list of time points (can have non-uniform intervals)\n",
        "  - k_vals: Dictionary of reaction kinetic constants needed for the reaction model\n",
        "\n",
        "  Returns:\n",
        "  - df: DataFrame containing the cncentration profile for each species over time\n",
        "  \"\"\"\n",
        "\n",
        "  # Extract Conc_0 from 'initial_conc' in params\n",
        "  initial_conc = np.array(list(Conc_0.values()))\n",
        "\n",
        "\n",
        "  n_steps=len(t)\n",
        "  n_species = len(initial_conc)\n",
        "\n",
        "\n",
        "  # Initialize the solution array to store each species' concentration at each time step\n",
        "  solution = np.zeros((n_steps, n_species))\n",
        "  #print(solution[0])\n",
        "  solution[0]= initial_conc # Initial conditions\n",
        "\n",
        "  # We create the ODE system as matrix with the rate constants dispossed as its coefficients\n",
        "  MCoef= create_ode_matrix(n_species, k_vals)\n",
        "\n",
        "\n",
        "\n",
        "  # Iterate through each time step using the function deriv_RK from funcionesGenerales\n",
        "  for i in range(1, n_steps):\n",
        "    current_t = t[i-1]\n",
        "    next_t =t[i]\n",
        "    current_y = solution[i-1]\n",
        "\n",
        "    # Here, calculate the time intercal (delta_t) dynamically\n",
        "    delta_t = next_t - current_t\n",
        "\n",
        "\n",
        "    # Use deriv_RK to calculate the next step, passing `f_deriv` as the first argument\n",
        "    solution[i] = current_y + delta_t * deriv_RK(\n",
        "        f_deriv, current_y, current_t, delta_t, MCoef\n",
        "    )\n",
        "\n",
        "  # df = pd.DataFrame(solution, index=t, columns= [\"A\", \"B\", \"C\", \"D\"]) # shape (time, species)\n",
        "  # Generate column names based on the number of species\n",
        "  column_names = [f\"{chr(65 + j)}\" for j in range(n_species)]  # 'A', 'B', 'C', ...\n",
        "\n",
        "  # Create the DataFrame without empty columns\n",
        "  df = pd.DataFrame(solution, index=t, columns=column_names)  # shape (time, species)\n",
        "  return df\n",
        "\n",
        "#concentration_matrix_rk4 = solv_conc_profile(initial_ks, deriv_conc, initial_conc, t=df.index)\n",
        "\n",
        "# Output the concentration matrix\n",
        "#concentration_matrix_rk4\n",
        "\n",
        "#def solv_conc_profile (params, f_deriv, Conc_0, t):\n",
        "def species_spectra (k_vals, f_deriv, Conc_0, t, abs, pathlength, method):\n",
        "  initial_conc = np.array(list(Conc_0.values()))\n",
        "\n",
        "\n",
        "  n_species = len(initial_conc)\n",
        "  # Extract the reaction model (fDeriv)\n",
        "  #model = k_vals.get('fDeriv')\n",
        "\n",
        "  C_profile = solv_conc_profile(k_vals, f_deriv, Conc_0,t)\n",
        "\n",
        "  #print(type(abs))\n",
        "  spectra = {}\n",
        "  if n_species == 2:\n",
        "    spectra['A'] = abs.iloc[0,:]/(C_profile.iloc[0,0] * pathlength)\n",
        "    spectra['B'] = abs.iloc[-1,:]/(C_profile.iloc[-1,1] * pathlength)\n",
        "  elif n_species == 3:\n",
        "    spectra['A'] = abs.iloc[0,:]/(C_profile.iloc[0,0] * pathlength )\n",
        "    spectra['B'] = None\n",
        "    spectra['C'] = abs.iloc[-1,:]/(C_profile.iloc[-1,2] * pathlength )\n",
        "\n",
        "\n",
        "    max_conc_B = C_profile.iloc[:,1].abs().max() # absolute maximum\n",
        "    #print(max_conc_B)\n",
        "    tB_label = C_profile.iloc[:,1].abs().idxmax() # This get the time not the positional idx\n",
        "    tB = C_profile.index.get_loc(tB_label)\n",
        "    #print(tB)\n",
        "    spectra['B'] = (1/(max_conc_B ))*(abs.iloc[tB,:]/(pathlength) -\n",
        "                                      spectra['A']*C_profile.iloc[tB,0]  -\n",
        "                                      spectra['C']*C_profile.iloc[tB,-1] )\n",
        "  elif n_species == 4:\n",
        "    spectra['A'] = abs.iloc[0,:]/(C_profile.iloc[0,0] * pathlength)\n",
        "    spectra['B'] = None\n",
        "    spectra['C'] = None\n",
        "    spectra['D'] = abs.iloc[-1,:]/(C_profile.iloc[-1,3] * pathlength)\n",
        "\n",
        "    max_conc_B = C_profile.iloc[:,1].abs().max() # absolute maximum\n",
        "    #print(max_conc_B)\n",
        "    tB_label = C_profile.iloc[:,1].abs().idxmax() # This get the time not the positional idx\n",
        "    tB = C_profile.index.get_loc(tB_label)\n",
        "    #print(tB)\n",
        "\n",
        "    max_conc_C = C_profile.iloc[:,2].abs().max() # absolute maximum\n",
        "\n",
        "    tC_label = C_profile.iloc[:,2].abs().idxmax() # This get the time not the positional idx\n",
        "    tC = C_profile.index.get_loc(tC_label)\n",
        "\n",
        "    factor = C_profile.iloc[tC,2]/(C_profile.iloc[tC,2]*C_profile.iloc[tB,1] -\n",
        "                                   C_profile.iloc[tB,2]*C_profile.iloc[tC,1])\n",
        "    mu = C_profile.iloc[tB,2]/C_profile.iloc[tC,2]\n",
        "\n",
        "    spectra['B']= factor *(abs.iloc[tB,:]/(pathlength) -\n",
        "                           spectra['A']*C_profile.iloc[tB,0] -\n",
        "                           spectra['D']*C_profile.iloc[tB,-1] -\n",
        "                           mu*(abs.iloc[tC,:]/(pathlength) -\n",
        "                               spectra['A']*C_profile.iloc[tC,0] -\n",
        "                               spectra['D']*C_profile.iloc[tC,-1]))\n",
        "    spectra['C'] =(1/C_profile.iloc[tC,2])* (abs.iloc[tC,:]/(pathlength) -\n",
        "                                        spectra['A']*C_profile.iloc[tC,0] -\n",
        "                                        spectra['B']*C_profile.iloc[tC,1] -\n",
        "                                        spectra['D']*C_profile.iloc[tC,-1] )\n",
        "  sol = pd.DataFrame.from_dict(spectra)\n",
        "  sol= sol.T\n",
        "\n",
        "  # Until here is the calculation of the spectra explicitly with assumptions\n",
        "\n",
        "  # Now we are going to use the implicit approach of the explicit approach above\n",
        "\n",
        "  if method ==\"Explicit\":\n",
        "    result=sol\n",
        "  elif method == \"Implicit\":\n",
        "     # Generalized code to find max value and corresponding index\n",
        "    max_indices = {}\n",
        "    for col in C_profile.columns:\n",
        "      # max_value = C_profile[col].max()\n",
        "      max_index = C_profile[col].idxmax() # Get the index of the max value\n",
        "      max_indices [col] = max_index\n",
        "\n",
        "    # Use the indices found to slice the DataFrames\n",
        "    indices =list(max_indices.values())\n",
        "    reduced_conc=C_profile.loc[indices]\n",
        "    reduced_abs =abs.loc[indices]\n",
        "    # Solve the system of equations C^-1*A = E\n",
        "    sol_imp = np.dot(np.linalg.inv(reduced_conc), reduced_abs)\n",
        "\n",
        "    # Assign alphabetical names to the indices (A, B, C, ...)\n",
        "    alphabet_indices = [chr(65 + i) for i in range(len(indices))]  # 65 is ASCII for 'A'\n",
        "    sol_imp=pd.DataFrame(sol_imp, index=alphabet_indices, columns=abs.columns)\n",
        "    sol_imp=sol_imp\n",
        "    result=sol_imp\n",
        "  elif method == \"Pseudo-inverse\":\n",
        "    # Now we are going to use the pseudoinverse of the concentration to estimate the spectroscopic species\n",
        "    # (extinction coefficients)\n",
        "    sol_ps=np.dot(np.linalg.pinv(C_profile),datos_approx_df)\n",
        "    alphabet_indices = [chr(65 + i) for i in range(len(C_profile.columns))]  # 65 is ASCII for 'A'\n",
        "    sol_ps=pd.DataFrame(sol_ps, index=alphabet_indices, columns=abs.columns)\n",
        "    sol_ps=sol_ps\n",
        "    result=sol_ps\n",
        "  else:\n",
        "    print(\"Input for method not valid\")\n",
        "\n",
        "\n",
        "  #return sol, sol_imp, sol_ps\n",
        "  return result\n",
        "#def species_spectra (params, f_deriv, Conc_0, t, abs, pathlength):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Model_spectra(k_vals,f_deriv, Conc_0, t, abs, pathlength, original_data, method, fitting = True):\n",
        "\n",
        "  n_species = len(np.array(list(Conc_0.values())))\n",
        "  #Conc_0= Conc_0 # with the \"*\" operator we unpack the values of the dictionary\n",
        "\n",
        "  #Solve for concentrations\n",
        "  C_matrix=solv_conc_profile(k_vals, f_deriv, Conc_0,t)\n",
        "  #print(f'C_matrix{type(C_matrix)}')\n",
        "\n",
        "  #Construct full extinction coefficient matrix\n",
        "  S_matrix= species_spectra (k_vals, f_deriv, Conc_0, t, abs, pathlength, method)\n",
        "  #print(f'S_matrix{type(S_matrix)}')\n",
        "  #print(\"Shape of C_matrix:\", C_matrix.shape)  # Concentration matrix\n",
        "  #print(\"Shape of S_matrix:\", S_matrix.shape)  # Extinction coefficient matrix\n",
        "\n",
        "  #Use Lambert-Beer Law to calculate predicted absorbance (n_Lambda x n_t)\n",
        "  D_model = pathlength * np.dot(C_matrix, S_matrix)\n",
        "\n",
        "  D_exp = abs\n",
        "  D_org = original_data\n",
        "  #print(\"Shape of D_exp:\", D_exp.shape)  # Experimental absorbance\n",
        "  #print(\"Shape of D_model:\", D_model.shape)  # Predicted absorbance\n",
        "\n",
        "  #Compute residuals (difference between experimental abd predicted absorbance)\n",
        "  residuals_denoised= D_exp - D_model\n",
        "  residuals= D_org - D_model\n",
        "  #print(f'residuals{type(residuals)}')\n",
        "  D_model_df =pd.DataFrame(D_model, index=D_exp.index, columns= D_exp.columns)\n",
        "\n",
        "  if fitting:\n",
        "    sol= D_model_df.values.flatten()\n",
        "  else:\n",
        "    sol = {\n",
        "    \"params\": k_vals,\n",
        "    \"Conc_0\": Conc_0,\n",
        "    \"pathlength\": pathlength,\n",
        "    \"n_species\": n_species,\n",
        "    \"D_orig\": original_data,\n",
        "    \"D_approx\": abs,\n",
        "    \"D_model\": D_model_df,\n",
        "    \"C_matrix\": C_matrix,\n",
        "    \"S_matrix\": S_matrix,\n",
        "    \"residuals\": residuals,\n",
        "    \"residuals_denoised\": residuals_denoised\n",
        "}\n",
        "  #return D_model_df\n",
        "  return sol\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upload files\n",
        "\n",
        "#@markdown Here you have the option to upload different combinations of files as mentioned in the README.\n",
        "\n",
        "\n",
        "# Upload the file and save in a dictionary\n",
        "uploaded=files.upload()\n",
        "\n",
        "# Obtain the uploaded file name from the dictionary\n",
        "\n",
        "\n",
        "file_names = list(uploaded.keys())\n",
        "\n",
        "\n",
        "\n",
        "datos, file_name= lee_espectro(file_names)\n",
        "#datos = pd.read_csv(file_name,skiprows=[0])\n",
        "\n",
        "\n",
        "datos"
      ],
      "metadata": {
        "id": "TTxq757Yinnq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Spectra plot\n",
        "# @markdown Plots Absorbance vs Wavelength and Absorbance vs Time.\n",
        "\n",
        "df=datos\n",
        "df_transposed = df.T # Rows to columns and columns to rows\n",
        "\n",
        "wavelength_plot_2D = create_spectra_plot(df_transposed, Title = f\"Absorbance vs Wavelength // {file_name}\",\n",
        "                                         x_axis=\"Wavelength (nm)\", y_axis =\"Absorbance\",\n",
        "                                         Legend = \"Time (s)\")\n",
        "\n",
        "time_plot_2D = create_spectra_plot(df, Title= f\"Absorbance vs Time // {file_name}\",\n",
        "                                   x_axis = \"Time (s)\", y_axis = \"Absorbance\", Legend = \"Wavelength (nm)\" )\n",
        "\n",
        "plots = [wavelength_plot_2D,time_plot_2D]\n",
        "\n",
        "\n",
        "# Create tabs to display both plots\n",
        "wavelength_panel = TabPanel(child=wavelength_plot_2D, title=\"Wavelength Plot\")\n",
        "time_panel = TabPanel(child=time_plot_2D, title=\"Time Plot\")\n",
        "\n",
        "tabs = Tabs(tabs=[wavelength_panel, time_panel])\n",
        "\n",
        "# Show the plots\n",
        "show(tabs)"
      ],
      "metadata": {
        "id": "jFJONcZ2L1rN",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Singular Value Determination (SVD) and Identification of the Significant Singular Values (SSV)\n",
        "# Now we need a method to determine the number of significant singular values\n",
        "\n",
        "#@markdown **SVD Calculation**\n",
        "#@markdown\n",
        "#@markdown Only check the box below if you want to visualize the Singular Values (Sigma),\n",
        "#@markdown the transpose matrix (U) and the right matrix (VT).\n",
        "Check_SVD = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ****\n",
        "#@markdown **SSV Determination**\n",
        "scree_plot_th = 0.9 #@param{type: \"number\"}\n",
        "\n",
        "entropy_threshold= 0.9 #@param {type: \"number\"}\n",
        "if entropy_threshold <0 or entropy_threshold>1:\n",
        "  entropy_threshold=0.85\n",
        "  print(\"Value inputted not valid, entropy_threshold has taken the preset value of 0.85\")\n",
        "\n",
        "\n",
        "# Convert the dataframe into a Numpy array for the svd to work\n",
        "datos_array= datos.to_numpy()\n",
        "datos_array\n",
        "# Save the columns and row names\n",
        "Times= datos.index\n",
        "Wavelengths=datos.columns\n",
        "\n",
        "\n",
        "\n",
        "# Performn SVD\n",
        "U, Sigma, Vt = svd(datos_array, full_matrices= False)\n",
        "\n",
        "\n",
        "# Display results\n",
        "\n",
        "U_df = pd.DataFrame(U)\n",
        "\n",
        "Sigma_df = pd.DataFrame(Sigma)\n",
        "\n",
        "Vt_df = pd.DataFrame(Vt)\n",
        "if Check_SVD:\n",
        "  print(\"U Matrix:\\n\",U_df)\n",
        "  print(\"Singular Values:\\n\",Sigma_df)\n",
        "  print(\"V Transpose Matrix:\\n\",Vt_df)\n",
        "else:\n",
        "  pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Determine the number of significant singular values using the scree plot method\n",
        "n_significant_scree_plot = scree_plot_with_fit(Sigma, scree_plot_th)\n",
        "print(f\"Number of significant singular values (Scree Plot Method): {n_significant_scree_plot['SSVs']}\")\n",
        "\n",
        "# Determine the number of significant singular values using entropy-based selection\n",
        "n_significant_entropy = entropy_selection(Sigma, entropy_threshold)\n",
        "print(f\"\\nNumber of significant singular values (Entropy Method): {n_significant_entropy}\")\n",
        "\n",
        "# Determine the number of significant singular values using the broken stick method\n",
        "n_significant_broken_stick = broken_stick_method(Sigma)\n",
        "print(f\"\\nNumber of significant singular values (Broken Stick Method): {n_significant_broken_stick['SSVs']} \\n\")\n",
        "\n",
        "\n",
        "plots = [n_significant_scree_plot['plot'], n_significant_broken_stick['plot']]\n",
        "\n",
        "\n",
        "# Create tabs to display both plots\n",
        "scree_plot_panel = TabPanel(child=n_significant_scree_plot['plot'], title=\"Scree Plot\")\n",
        "broken_stick_panel = TabPanel(child=n_significant_broken_stick['plot'], title=\"Broken Stick Plot\")\n",
        "\n",
        "tabs = Tabs(tabs=[scree_plot_panel, broken_stick_panel])\n",
        "\n",
        "# Show the plots\n",
        "show(tabs)\n"
      ],
      "metadata": {
        "id": "_1zcoW3sh-ek",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dimensionality reduction and Matrix Approximation\n",
        "\n",
        "#@markdown **Number of Significant Singular Values (SSVs):**\n",
        "SSVs = 3 #@param {type:\"number\"}\n",
        "#SSV = n_significant_entropy #@param [n_significant_entropy, n_significant_broken, n_significant_manual]\n",
        "\n",
        "\n",
        "\n",
        "datos_approx = matrix_approximation(datos_array, SSVs)\n",
        "#print(datos_approx.shape)\n",
        "#print(datos_array.shape)\n",
        "print(f\"Approximation of the original data, using the first {SSVs} SSVs: \\n\")\n",
        "datos_approx_df= pd.DataFrame(datos_approx, index=Times, columns= Wavelengths)\n",
        "datos_approx_df"
      ],
      "metadata": {
        "id": "4E_Muw2GYkqM",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Spectra plot\n",
        "# @markdown Plots Absorbance vs Wavelength and Absorbance vs Time.\n",
        "\n",
        "df=datos_approx_df\n",
        "df_transposed = df.T # Rows to columns and columns to rows\n",
        "\n",
        "wavelength_plot_2D = create_spectra_plot(df_transposed, Title = f\"Absorbance vs Wavelength // {file_name}\",\n",
        "                                         x_axis=\"Wavelength (nm)\", y_axis =\"Absorbance\",\n",
        "                                         Legend = \"Time (s)\")\n",
        "\n",
        "time_plot_2D = create_spectra_plot(df, Title= f\"Absorbance vs Time // {file_name}\",\n",
        "                                   x_axis = \"Time (s)\", y_axis = \"Absorbance\", Legend = \"Wavelength (nm)\" )\n",
        "\n",
        "plots = [wavelength_plot_2D,time_plot_2D]\n",
        "\n",
        "\n",
        "# Create tabs to display both plots\n",
        "wavelength_panel = TabPanel(child=wavelength_plot_2D, title=\"Wavelength Plot\")\n",
        "time_panel = TabPanel(child=time_plot_2D, title=\"Time Plot\")\n",
        "\n",
        "tabs = Tabs(tabs=[wavelength_panel, time_panel])\n",
        "\n",
        "# Show the plots\n",
        "show(tabs)"
      ],
      "metadata": {
        "id": "EErjJoVQ_uZf",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Reaction Model Parameters { run: \"auto\" }\n",
        "\n",
        "#@markdown **Number of species:**\n",
        "n_species = 3 #@param{type: \"slider\", min:2, max:4, step:1}\n",
        "\n",
        "#@markdown **Pathlength of the cuvette (cm):**\n",
        "pathlength = 1 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown ****\n",
        "# Initial concentrations of species\n",
        "#@markdown **Initial Concentrations (μM):**\n",
        "A0 = 0.0 #@param {type:\"number\"}\n",
        "\n",
        "B0 = 0.0 #@param {type:\"number\"}\n",
        "\n",
        "C0 = 0.0 #@param {type:\"number\"}\n",
        "\n",
        "D0 = 0.0 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown ****\n",
        "\n",
        "# Enter the rate constants for the reactions\n",
        "#@markdown **Kinetic Rates (1/s):**\n",
        "k1 = 0 #@param {type:\"number\"}\n",
        "k1_fixed = True  # @param {type: \"boolean\"}\n",
        "#@markdown\n",
        "k_1 = 0 #@param {type:\"number\"}\n",
        "k_1_fixed = True  # @param {type: \"boolean\"}\n",
        "#@markdown\n",
        "k2 = 0 #@param {type:\"number\"}\n",
        "k2_fixed = True  # @param {type: \"boolean\"}\n",
        "#@markdown\n",
        "k_2 = 0 #@param {type:\"number\"}\n",
        "k_2_fixed = True  # @param {type: \"boolean\"}\n",
        "#@markdown\n",
        "k3 = 0 #@param {type:\"number\"}\n",
        "k3_fixed = True  # @param {type: \"boolean\"}\n",
        "#@markdown\n",
        "k_3 = 0 #@param {type:\"number\"}\n",
        "k_3_fixed = True  # @param {type: \"boolean\"}\n",
        "\n",
        "#@markdown ****\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define rate constants and their fixed status in a dictionary\n",
        "rate_constants_data = {\n",
        "    'k1': (k1, k1_fixed),\n",
        "    'k_1': (k_1, k_1_fixed),\n",
        "    'k2': (k2, k2_fixed),\n",
        "    'k_2': (k_2, k_2_fixed),\n",
        "    'k3': (k3, k3_fixed),\n",
        "    'k_3': (k_3, k_3_fixed)\n",
        "}\n",
        "\n",
        "# Initialize dictionaries for fixed and variable rate constants (these last ones are to be optimized later)\n",
        "fixed_ks = {}\n",
        "variable_ks = {}\n",
        "\n",
        "# Loop over each rate constant and classify it into fixed or variable\n",
        "for rate, (value, is_fixed) in rate_constants_data.items():\n",
        "    if is_fixed:\n",
        "        fixed_ks[rate] = value\n",
        "    else:\n",
        "        variable_ks[rate] = value\n",
        "\n",
        "# Output for debugging or checking\n",
        "#print(\"Fixed Rate Constants:\", fixed_ks)\n",
        "#print(\"Variable Rate Constants:\", variable_ks)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define initial concentrations and their fixed status in a dictionary\n",
        "concentration_data= {\n",
        "    'A0': A0,\n",
        "    'B0': B0,\n",
        "    'C0': C0,\n",
        "    'D0': D0\n",
        "}\n",
        "\n",
        "# Slice the dictionary using a subset of its keys\n",
        "species_list = ['A0', 'B0', 'C0', 'D0'][:n_species]\n",
        "# Here we get the sliced dictionary\n",
        "initial_conc = {key:concentration_data[key] for key in species_list}\n",
        "\n",
        "#\n",
        "\n",
        "# Output for debugging or checking\n",
        "#print(\"Fixed Concentrations:\", fixed_conc)\n",
        "#print(\"Variable Concentrations:\", variable_conc)\n",
        "\n",
        "\n",
        "# Output of the fixed and variable rate constants\n",
        "print(\"Fixed Rate Constants:\")\n",
        "for key, value in fixed_ks.items():\n",
        "    print(f\"{key} = {value}\")\n",
        "\n",
        "print(\"\\nVariable Rate Constants:\")\n",
        "for key, value in variable_ks.items():\n",
        "    print(f\"{key} = {value}\")\n",
        "\n",
        "print(\"\\nInitial Concentrations:\")\n",
        "for key, value in initial_conc.items():\n",
        "    print(f\"{key} = {value}\")\n",
        "\n",
        "# Group both rate constant dictionaries into one\n",
        "initial_ks = {**fixed_ks, **variable_ks}"
      ],
      "metadata": {
        "id": "81m0Wl1HXdO8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Procesa\n",
        "\n",
        "#@markdown Method use for estimating the spectroscopic species:\n",
        "Method=\"Pseudo-inverse\" #@param[\"Pseudo-inverse\", \"Explicit\", \"Implicit\"]\n",
        "\n",
        "#@markdown The parameter above heavily influences the goodness of fitting.\n",
        "#@markdown Select **\"Pseudo-inverse\"** for the best fitting, however, it requires a reasonable\n",
        "#@markdown first estimation of the rate constant. Otherwise, select **\"Explicit\"** to obtain an initial\n",
        "#@markdown idea of the possible magnitude of the rate constants.\n",
        "initial_params= {**initial_ks }\n",
        "initial_params_var= {**variable_ks}\n",
        "\n",
        "\n",
        "#initial_params['n_species']=n_species # This dictionary contains the rate constants and the number of species\n",
        "#initial_params['pathlength']= pathlength # We add pathlength to the dictionary to streamline the parameters onto\n",
        "# a single dictionary\n",
        "\n",
        "# Prepare the list of the parameters names to be opimized\n",
        "nombrParVar = list(initial_params_var.keys())\n",
        "\n",
        "# Independent values (time and wavelength) and dependent values (Absorbance)\n",
        "fKwargs = dict(t=datos_approx_df.index.values,\n",
        "               f_deriv=deriv_conc,\n",
        "               Conc_0=initial_conc,\n",
        "               abs=datos_approx_df,\n",
        "               pathlength=pathlength,\n",
        "               original_data= datos,\n",
        "               method= Method,\n",
        "               fitting=True\n",
        "                )\n",
        "\n",
        "abs= datos_approx_df\n",
        "\n",
        "\n",
        "#cotaInf = [0 for param in nombrParVar]\n",
        "#cotaSup= [2 if param =='Ro' else np.inf for param in nombrParVar]\n",
        "\n",
        "sol=procesa(argLeastSquares = argLeastSquares,\n",
        "            dictParEstim = initial_params,\n",
        "            nombrParVar = nombrParVar,\n",
        "            f = Model_spectra,\n",
        "            fKwargs = fKwargs,\n",
        "            Y = datos_approx_df.values.flatten()\n",
        "            )"
      ],
      "metadata": {
        "id": "n3cByiA-Hzur",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model's plots\n",
        "# @markdown This cell create plots with the information obtained by the model and\n",
        "#@markdown the optimization process.\n",
        "\n",
        "ad_parameters= sol['parAjustados']\n",
        "Model= Model_spectra( ad_parameters, deriv_conc, initial_conc,datos_approx_df.index, datos_approx_df, pathlength,datos, method= Method, fitting=False,)\n",
        "df_e=Model['D_model']\n",
        "\n",
        "\n",
        "df_e_transposed = df_e.T # Rows to columns and columns to rows\n",
        "\n",
        "wavelength_plot_2D = create_spectra_plot(df_e_transposed, Title = f\"Absorbance vs Wavelength // {file_name}\",\n",
        "                                         x_axis=\"Wavelength (nm)\", y_axis =\"Absorbance\",\n",
        "                                         Legend = \"Time (s)\")\n",
        "\n",
        "time_plot_2D = create_spectra_plot(df_e, Title= f\"Absorbance vs Time // {file_name}\",\n",
        "                                   x_axis = \"Time (s)\", y_axis = \"Absorbance\", Legend = \"Wavelength (nm)\" )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_conc= Model['C_matrix']\n",
        "df_conc\n",
        "\n",
        "conc_profile_plot = create_spectra_plot(df_conc, Title = f\"Concentration over time // {file_name}\",\n",
        "                                         x_axis=\"Time (s)\", y_axis =\"Concentration (μM)\",\n",
        "                                         Legend = \"Species\")\n",
        "#show(conc_profile_plot)\n",
        "\n",
        "df_spectra=Model['S_matrix'].T\n",
        "df_spectra\n",
        "\n",
        "\n",
        "spectra_species_plot = create_spectra_plot(df_spectra, Title = f\"Spectroscopic species // {file_name}\",\n",
        "                                         x_axis=\"Wavelength (nm)\", y_axis =\"Extinction Coefficient (1/(μM*cm))\",\n",
        "                                         Legend = \"Species\")\n",
        "#show(spectra_profile_plot)\n",
        "\n",
        "# Original - Model\n",
        "df_res=Model['residuals']\n",
        "df_res_transposed =df_res.T\n",
        "\n",
        "wavelength_res_plot_2D = create_spectra_plot(df_res_transposed, Title = f\"Absorbance vs Wavelength // {file_name}\",\n",
        "                                         x_axis=\"Wavelength (nm)\", y_axis =\"Absorbance\",\n",
        "                                         Legend = \"Time (s)\")\n",
        "time_res_plot_2D = create_spectra_plot(df_res, Title= f\"Absorbance vs Time // {file_name}\",\n",
        "                                   x_axis = \"Time (s)\", y_axis = \"Absorbance\", Legend = \"Wavelength (nm)\" )\n",
        "\n",
        "# Denoised - Model\n",
        "df_res_d=Model['residuals_denoised']\n",
        "df_res_d_transposed =df_res_d.T\n",
        "\n",
        "wavelength_res_d_plot_2D = create_spectra_plot(df_res_d_transposed, Title = f\"Absorbance vs Wavelength // {file_name}\",\n",
        "                                         x_axis=\"Wavelength (nm)\", y_axis =\"Absorbance\",\n",
        "                                         Legend = \"Time (s)\")\n",
        "time_res_d_plot_2D = create_spectra_plot(df_res_d, Title= f\"Absorbance vs Time // {file_name}\",\n",
        "                                   x_axis = \"Time (s)\", y_axis = \"Absorbance\", Legend = \"Wavelength (nm)\" )\n",
        "\n",
        "\n",
        "plots = [wavelength_plot_2D,wavelength_res_plot_2D,time_plot_2D,time_res_plot_2D, conc_profile_plot, spectra_species_plot,wavelength_res_d_plot_2D, time_res_d_plot_2D ]\n",
        "\n",
        "\n",
        "# Create tabs to display both plots\n",
        "wavelength_panel = TabPanel(child=wavelength_plot_2D, title=\"Wavelength Plot\")\n",
        "wavelength_res_panel = TabPanel(child=wavelength_res_plot_2D, title=\"Wavelength Residuals Plot\")\n",
        "wavelength_res_d_panel = TabPanel(child=wavelength_res_d_plot_2D, title=\"Wavelength Residuals Denoised Plot\")\n",
        "\n",
        "time_panel = TabPanel(child=time_plot_2D, title=\"Time Plot\")\n",
        "time_res_panel = TabPanel(child=time_res_plot_2D, title=\"Time Residuals Plot\")\n",
        "time_res_d_panel = TabPanel(child=time_res_d_plot_2D, title=\"Time Residuals Denoised Plot\")\n",
        "\n",
        "conc_profile_panel= TabPanel(child=conc_profile_plot, title=\"Concentration Profile\")\n",
        "spectra_species_panel = TabPanel(child= spectra_species_plot, title= \"Spectroscopic Species\")\n",
        "\n",
        "tabs = Tabs(tabs=[wavelength_panel, time_panel, conc_profile_panel, spectra_species_panel, wavelength_res_panel,wavelength_res_d_panel, time_res_panel, time_res_d_panel ])\n",
        "\n",
        "# Show the plots\n",
        "show(tabs)"
      ],
      "metadata": {
        "id": "ng98Lzll6Qkg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Export results\n",
        "\n",
        "# @markdown Write the name for the zip file that contains the inputted and produced data.\n",
        "\n",
        "# Initial experimental data\n",
        "Model['D_orig'].to_csv('Original_experimental_data.csv', index=True)\n",
        "\n",
        "# Denoised experimental data\n",
        "Model['D_approx'].to_csv('Denoised_experimental_data.csv', index=True)\n",
        "\n",
        "# Modeled data\n",
        "Model['D_model'].to_csv('Modeled_data.csv', index=True)\n",
        "\n",
        "# Residuals from Original - Modeled data\n",
        "Model['residuals'].to_csv('Residuals_OrigMod.csv', index=True)\n",
        "\n",
        "# Residuals from Denoised - Modeled data\n",
        "Model['residuals_denoised'].to_csv('Residuals_DenMod.csv', index=True)\n",
        "\n",
        "# Concentration profile\n",
        "Model['C_matrix'].to_csv('Concentration_profile.csv', index=True)\n",
        "\n",
        "# Spectroscopic species\n",
        "Model['S_matrix'].to_csv('Spectroscopic_species.csv', index=True)\n",
        "\n",
        "\n",
        "\n",
        "# Create a CSV\n",
        "with open('Fitting_result.csv', mode='w', newline='') as file:\n",
        "  writer = csv.writer(file)\n",
        "\n",
        "  #Add blank rows\n",
        "  writer.writerow([''] * 7)\n",
        "  writer.writerow([''] * 7)\n",
        "\n",
        "  # n_species & pathlength Section\n",
        "  writer.writerow(['n_species', n_species])\n",
        "  writer.writerow(['pathlength (cm)', pathlength])\n",
        "  writer.writerow([''] * 7)\n",
        "\n",
        "  # Initial concentrations Section\n",
        "  writer.writerow(['INITIAL CONCENTRATIONS:'])\n",
        "  writer.writerow(['A0', Model['Conc_0']['A0']\n",
        "                   if 'AO' in Model['Conc_0'] else ''])\n",
        "  writer.writerow(['B0', Model['Conc_0']['B0']\n",
        "                   if 'BO' in Model['Conc_0'] else ''])\n",
        "  writer.writerow(['C0', Model['Conc_0']['C0']\n",
        "                   if 'CO' in Model['Conc_0'] else ''])\n",
        "  writer.writerow(['D0', Model['Conc_0']['D0']\n",
        "                   if 'DO' in Model['Conc_0'] else ''])\n",
        "\n",
        "  # Headers for the rate constants results\n",
        "  writer.writerow(['INITIAL ks', '', 'ADJUSTED ks','', 'STD ks'] )\n",
        "  #k1 row\n",
        "  writer.writerow(['k1', initial_ks['k1'], '', 'k1', sol['parAjustados']['k1'], '',\n",
        "        'k1_std', sol['sdPar'].get('k1_std', '') if 'k1' in variable_ks else ''])  # Display k1_std only if it is a variable parameter\n",
        "  #k_1 row\n",
        "  writer.writerow(['k_1', initial_ks['k_1'], '', 'k_1', sol['parAjustados']['k_1'], '',\n",
        "        'k_1_std', sol['sdPar'].get('k_1_std', '') if 'k_1' in variable_ks else ''])  # Display k_1_std only if it is a variable parameter\n",
        "\n",
        "  #k2 row\n",
        "  writer.writerow(['k2', initial_ks['k2'], '', 'k2', sol['parAjustados']['k2'], '',\n",
        "        'k2_std', sol['sdPar'].get('k2_std', '') if 'k2' in variable_ks else ''])  # Display k2_std only if it is a variable parameter\n",
        "  #k_2 row\n",
        "  writer.writerow(['k_2', initial_ks['k_2'], '', 'k_2', sol['parAjustados']['k_2'], '',\n",
        "        'k_2_std', sol['sdPar'].get('k_2_std', '') if 'k_2' in variable_ks else ''])  # Display k_2_std only if it is a variable parameter\n",
        "\n",
        "  #k3 row\n",
        "  writer.writerow(['k3', initial_ks['k3'], '', 'k3', sol['parAjustados']['k3'], '',\n",
        "        'k3_std', sol['sdPar'].get('k3_std', '') if 'k3' in variable_ks else ''])  # Display k3_std only if it is a variable parameter\n",
        "  #k_3 row\n",
        "  writer.writerow(['k_3', initial_ks['k_3'], '', 'k_3', sol['parAjustados']['k_3'], '',\n",
        "        'k_3_std', sol['sdPar'].get('k_3_std', '') if 'k_3' in variable_ks else ''])  # Display k_3_std only if it is a variable parameter\n",
        "\n",
        "\n",
        "  # Add blank rows\n",
        "  writer.writerow([''] * 7)\n",
        "  writer.writerow([''] * 7)\n",
        "\n",
        "  # R2 Section\n",
        "  writer.writerow(['R2'])\n",
        "  writer.writerow(['R2', sol['R2']])\n",
        "  writer.writerow([''] * 7)\n",
        "  writer.writerow([''] * 7)\n",
        "\n",
        "  # Detalles Section\n",
        "  writer.writerow(['Details'])\n",
        "\n",
        "  # x values (convert floats to strings to concatenate with 'x')\n",
        "  writer.writerow(['x'] + [str(x) for x in sol['detalles']['x']])\n",
        "\n",
        "  # cost\n",
        "  writer.writerow(['cost', sol['detalles']['cost']])\n",
        "\n",
        "  # Add blank rows\n",
        "  writer.writerow([''] * 7)\n",
        "\n",
        "  # grad values (convert floats to strings)\n",
        "  writer.writerow(['grad'] + [str(g) for g in sol['detalles']['grad']])\n",
        "\n",
        "  # optimality\n",
        "  writer.writerow(['optimality', sol['detalles']['optimality']])\n",
        "\n",
        "  # active_mask (convert integers to strings)\n",
        "  writer.writerow(['active_mask'] + [str(a) for a in sol['detalles']['active_mask']])\n",
        "\n",
        "  # nfev, njev, status, message, success\n",
        "  writer.writerow(['nfev', sol['detalles']['nfev']])\n",
        "  writer.writerow(['njev', sol['detalles']['njev']])\n",
        "  writer.writerow(['status', sol['detalles']['status']])\n",
        "  writer.writerow(['message', sol['detalles']['message']])\n",
        "  writer.writerow(['success', sol['detalles']['success']])\n",
        "\n",
        "  # fun values (split into multiple rows)\n",
        "  fun_values = sol['detalles']['fun']\n",
        "  for row in fun_values:\n",
        "    writer.writerow(['fun'] + [str(row)])\n",
        "\n",
        "  # Add blank rows\n",
        "  writer.writerow([''] * 7)\n",
        "\n",
        "  # jac values (split into multiple rows)\n",
        "  for row in sol['detalles']['jac']:\n",
        "      writer.writerow(['jac'] + [str(v) for v in row])\n",
        "\n",
        "  # Add blank rows\n",
        "  writer.writerow([''] * 7)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Then we proceed to save all the files and compressed them into a zip file\n",
        "\n",
        "# Take the current date and hour ()\n",
        "current_time = datetime.now().strftime(\"%d%m%Y%H%M%S\")\n",
        "\n",
        "# Define the prefix and create the complete name of the zip file\n",
        "name =\"spectra_\" #@param {type: \"string\"}\n",
        "zip_filename = f\"{name}{current_time}.zip\"\n",
        "\n",
        "# Create a zip file with the name written\n",
        "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "    # Add CSV files\n",
        "    zipf.write('Original_experimental_data.csv')\n",
        "    zipf.write('Denoised_experimental_data.csv')\n",
        "    zipf.write('Modeled_data.csv')\n",
        "    zipf.write ('Residuals_OrigMod.csv')\n",
        "    zipf.write ('Residuals_DenMod.csv')\n",
        "    zipf.write ('Concentration_profile.csv')\n",
        "    zipf.write ('Spectroscopic_species.csv')\n",
        "    zipf.write ('Fitting_result.csv')\n",
        "\n",
        "\n",
        "\n",
        "# Download the zipped file\n",
        "files.download(zip_filename)"
      ],
      "metadata": {
        "id": "AmT3Zt6_TLLG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}