{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unizar-flav/KiPaD/blob/master/KiPaD_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uahnpGtYp8-"
      },
      "source": [
        "# Kinetic Parameters Determination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8EAFRoSbpKG",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Environment\n",
        "#@markdown The user only needs to run this cell once regardless of the number of datasets to be evaluated.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import zipfile # Necessary to compress the files into zip\n",
        "import csv\n",
        "from matplotlib.colors import to_hex  # Import the to_hex function\n",
        "\n",
        "\n",
        "from scipy.linalg import svd\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "from bokeh.io import output_notebook, show, export_png\n",
        "from bokeh.plotting import figure, output_file, save, show\n",
        "\n",
        "from bokeh.palettes import linear_palette, Viridis256\n",
        "from bokeh.palettes import Category20\n",
        "from bokeh.models import Button, CustomJS, TabPanel, Tabs, Legend, Span, Label, Select\n",
        "from bokeh.layouts import column\n",
        "from bokeh.transform import linear_cmap\n",
        "output_notebook()\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "from datetime import datetime\n",
        "\n",
        "!git clone https://github.com/unizar-flav/KiPaD.git\n",
        "from KiPaD.funcionesGenerales import  procesa, argLeastSquares, deriv_RK\n",
        "\n",
        "\n",
        "# Function lee_espectro\n",
        "def lee_espectro(nombrFichs, tag =\"_t\",skip_rows= 0):\n",
        "  df_list = [] # Initialize an empty list to store DataFrames\n",
        "\n",
        "  for fich in nombrFichs:\n",
        "    #Read each file into a DataFrame\n",
        "    temp_df = pd.read_csv(fich, skiprows=[skip_rows], index_col = 0)\n",
        "    df_list.append(temp_df) # Append each DataFrame to the list\n",
        "\n",
        "  # Concatenate all DataFrames into one\n",
        "  df = pd.concat(df_list)\n",
        "\n",
        "  # Sort the resulting DataFrame by index\n",
        "  df = df.sort_index()\n",
        "\n",
        "  # Let's get a name for the plots to follow which data was uploaded\n",
        "  main = next((fich for fich in nombrFichs if tag in fich), None)\n",
        "  return df, main\n",
        "\n",
        "\n",
        "# Plot function\n",
        "def create_plot(df, Title, x_axis, y_axis, Legend, width=1200, height=700):\n",
        "    # Create a figure\n",
        "    p = figure(title=Title,\n",
        "               x_axis_label=x_axis,\n",
        "               y_axis_label=y_axis,\n",
        "               width=width, height=height)\n",
        "\n",
        "    # Define font sizes for the title, axes, and labels\n",
        "    p.title.text_font_size = '20pt'\n",
        "    p.xaxis.axis_label_text_font_size = '16pt'\n",
        "    p.yaxis.axis_label_text_font_size = '16pt'\n",
        "    p.xaxis.major_label_text_font_size = '12pt'\n",
        "    p.yaxis.major_label_text_font_size = '12pt'\n",
        "\n",
        "    # Generate a color palette using Viridis256\n",
        "    #n_lines = len(df.columns)\n",
        "    #colors = linear_palette(Viridis256, n_lines)\n",
        "\n",
        "    # Generate a custom color palette using matplotlib's colormap\n",
        "    n_lines = len(df.columns)\n",
        "    cmap = plt.get_cmap(\"rainbow\")  # Use the 'rainbow' colormap for visible spectrum\n",
        "    colors = [to_hex(cmap(i)) for i in np.linspace(0, 1, n_lines)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    indices = pd.to_numeric(df.index)\n",
        "    # Plot each column as a line\n",
        "    for idx, col in enumerate(df.columns):\n",
        "        p.line(indices, df[col], legend_label=str(col), line_width=2, color=colors[idx])\n",
        "        #p.line(pd.to_numeric(df.index), df[col], legend_label=str(col), line_width=2, color=colors[idx])\n",
        "    # Customize the legend\n",
        "    p.legend.title = Legend\n",
        "    p.legend.location = \"top_right\"\n",
        "    p.legend.click_policy = \"hide\"  # Allows hiding lines by clicking their labels\n",
        "    p.toolbar_location = \"below\"\n",
        "    p.legend.visible = False # Initially hide the legend\n",
        "    p.legend.label_text_font_size = '12pt'\n",
        "    p.legend.title_text_font_size = '14pt'\n",
        "\n",
        "    # Create a button to toggle the legend visibility\n",
        "    button = Button(label=\"Toggle Legend\", button_type= \"success\")\n",
        "\n",
        "    # Custom JavaScript to toggle legend visibility\n",
        "    button.js_on_click(CustomJS(args=dict(legend=p.legend[0]), code= \"\"\"\n",
        "    legend.visible = !legend.visible;  // Toggle the visibility\n",
        "\"\"\"))\n",
        "\n",
        "    # Return the plot object and button as a column layout\n",
        "    return column(p,button)\n",
        "\n",
        "def slice_dataset(df, t_start=None, t_end=None, wave_start=None, wave_end=None):\n",
        "    \"\"\"\n",
        "    Slices a dataset row-wise and column-wise.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The input dataset.\n",
        "    - t_start (float, optional): The starting value for row slicing.\n",
        "    - t_end (float, optional): The ending value for row slicing.\n",
        "    - wave_start (float, optional): The starting value for column slicing.\n",
        "    - wave_end (float, optional): The ending value for column slicing.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: The sliced dataset.\n",
        "    \"\"\"\n",
        "    # Ensure t_start and t_end are provided\n",
        "    if t_start is not None and t_end is not None:\n",
        "        # Find the index of the target time in the df.index\n",
        "        row_start = (np.abs(df.index - t_start)).argmin()\n",
        "        row_end = (np.abs(df.index - t_end)).argmin()\n",
        "    else:\n",
        "        row_start, row_end = None, None\n",
        "\n",
        "    # Ensure wave_start and wave_end are provided\n",
        "    if wave_start is not None and wave_end is not None:\n",
        "        # Convert df.columns to float\n",
        "        columns = df.columns.astype(float)  # Converts Index to array of floats\n",
        "        col_start = (np.abs(columns - wave_start)).argmin()\n",
        "        col_end = (np.abs(columns - wave_end)).argmin()\n",
        "    else:\n",
        "        col_start, col_end = None, None\n",
        "\n",
        "    # Slice the rows\n",
        "    if row_start is not None or row_end is not None:\n",
        "        df = df.iloc[row_start:row_end]\n",
        "\n",
        "    # Slice the columns\n",
        "    if col_start is not None or col_end is not None:\n",
        "        df = df.iloc[:, col_start:col_end]\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "# Scree Plot Method, with an elbow selection criterion based on the regression coefficient\n",
        "\n",
        "\n",
        "def scree_plot_with_fit(singular_values, threshold, width=800, height=600):\n",
        "    '''\n",
        "    Plots scree plot of singular values and determine significant values using a linear fit.\n",
        "\n",
        "    Parameters:\n",
        "        singular_values (array-like): Array of singular values.\n",
        "        threshold (float): Regression coefficient threshold (between 0 and 1) for linear fit.\n",
        "\n",
        "    Returns:\n",
        "        dict: Number of significant singular values and a Bokeh plot.\n",
        "    '''\n",
        "\n",
        "    n_values = len(singular_values)\n",
        "    SSVs = 0  # Number of significant singular values to keep\n",
        "\n",
        "    # Initialize Bokeh figure\n",
        "    p = figure(title = \" Scree Plot with Linear Fit\",\n",
        "               x_axis_label=\"Singular Value Index\",\n",
        "               y_axis_label= \"Singular Values\",\n",
        "               width= width, height=height)\n",
        "    p.title.text_font_size = '20pt'\n",
        "    p.xaxis.axis_label_text_font_size = '16pt'\n",
        "    p.yaxis.axis_label_text_font_size = '16pt'\n",
        "    p.xaxis.major_label_text_font_size = '12pt'\n",
        "    p.yaxis.major_label_text_font_size = '12pt'\n",
        "\n",
        "    # Plot singular values\n",
        "    indices = np.arange(1, n_values +1)\n",
        "    p.scatter(indices, singular_values, size=8, color='blue', legend_label= \"Singular Values\")\n",
        "\n",
        "    # Placeholder variables for the inear fit line data\n",
        "    X_final, y_final_pred = None, None\n",
        "\n",
        "\n",
        "\n",
        "    # Iterate through singular values, trying linear fits\n",
        "    for i in range(2, n_values + 1):  # Start with at least two points for linear regression\n",
        "        X = np.arange(1, i + 1).reshape(-1, 1)\n",
        "        y = singular_values[:i]\n",
        "\n",
        "        # Perform linear regression\n",
        "        model = LinearRegression().fit(X, y)\n",
        "        r_squared = model.score(X, y)  # Get the R^2 (regression coefficient)\n",
        "\n",
        "        # If the fit quality falls below the threshold, stop\n",
        "        if r_squared < threshold:\n",
        "            SSVs = i - 1\n",
        "            break\n",
        "        else:\n",
        "            SSVs = i\n",
        "        # Update the final data for the significant linear fit\n",
        "        X_final = np.arange(1, SSVs + 1).reshape(-1,1)\n",
        "        y_final_pred = model.predict(X_final)\n",
        "\n",
        "    # Plot the linear fit up to the last significant singular value\n",
        "    p.line(X_final.flatten(), y_final_pred, line_width = 2, color= \"red\", line_dash= \"dashed\",\n",
        "                           legend_label= \"Linear Fit\")\n",
        "\n",
        "    # Add a vertical line to mark the cutoff for significant values\n",
        "    cutoff_line = Span(location= SSVs, dimension= 'height', line_color=\"green\", line_dash=\"dashed\")\n",
        "    p.add_layout(cutoff_line)\n",
        "\n",
        "    # Add a label indicating the cutoff\n",
        "    cutoff_label = Label(x=SSVs, y=singular_values[SSVs - 1], text = f'Significant Count = {SSVs}',\n",
        "                         text_color='green', y_offset=10)\n",
        "    p.add_layout(cutoff_label)\n",
        "\n",
        "    # Customize Legend and toolbar\n",
        "    p.legend.title = \"Legend\"\n",
        "    p.legend.location = \"top_right\"\n",
        "    p.legend.click_policy = \"hide\"\n",
        "    p.toolbar_location = \"below\"\n",
        "\n",
        "    # Add a toogle button to control the legend visibility\n",
        "    button = Button(label = \"Toggle Legend\", button_type = \"success\")\n",
        "    button.js_on_click(CustomJS(args=dict(legend=p.legend[0]),\n",
        "                                code=\"\"\"\n",
        "                                legend.visible = ! legend.visible;\n",
        "                                \"\"\"))\n",
        "\n",
        "    #Show the plot with the toggle button\n",
        "    plot=column(p,button)\n",
        "    sol={'SSVs':SSVs, \"plot\":plot}\n",
        "    return sol\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def entropy_selection(singular_values, entropy_threshold):\n",
        "\n",
        "    '''\n",
        "    Entropy based method to determine the number of significant singular values.\n",
        "\n",
        "    Parameters:\n",
        "        singular_values (array-like): Array of singular values.\n",
        "        threshold (float): PENDING\n",
        "    Returns:\n",
        "        dict: Number of significant singular values and a Bokeh plot.\n",
        "    '''\n",
        "    total_energy = np.sum(singular_values ** 2)\n",
        "\n",
        "    # Calculate normalized singular values (f_j)\n",
        "    f_j = singular_values ** 2 / total_energy\n",
        "\n",
        "    # Calculate entropy\n",
        "    entropy_val = np.sum(f_j * np.log(f_j)) / np.log(len(singular_values))\n",
        "\n",
        "    # Uncomment the line below in order to check the entropy of the singular values:\n",
        "    #print(f\"\\t Entropy of singular values: {entropy_val:.4f}\")\n",
        "\n",
        "    # Calculate cumulative entropy for each k\n",
        "    cumulative_entropy = np.zeros(len(singular_values))\n",
        "    for k in range(len(singular_values)):\n",
        "        ff = f_j[:k+1]\n",
        "        cumulative_entropy[k]=np.sum(ff*np.log(ff)/np.log(len(singular_values)))\n",
        "    percentage=cumulative_entropy/entropy_val\n",
        "    #print(percentage)\n",
        "\n",
        "    # Find the smallest index k such that cumulative entropy meets the threshold\n",
        "    significant_indices = np.where(percentage >= entropy_threshold)[0]\n",
        "\n",
        "    if len(significant_indices) == 0:\n",
        "        return 0  # Return 0 if no significant indices found\n",
        "    else:\n",
        "        return significant_indices[0] + 1  # Return the number of significant components\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Broken Stick Method\n",
        "def broken_stick_method(singular_values, width=800, height=600):\n",
        "    '''\n",
        "    Broken Stick Method to determine the number of significant singular values.\n",
        "\n",
        "    Parameters:\n",
        "        singular_values (array-like): Array of singular values.\n",
        "\n",
        "    Returns:\n",
        "        dict: Number of significant singular values and a Bokeh plot.\n",
        "    '''\n",
        "    k = len(singular_values)\n",
        "\n",
        "    # Calculate the broken stick values\n",
        "    broken_stick = np.zeros(k)\n",
        "    for i in range(1, k + 1):\n",
        "        broken_stick[i - 1] = (1 / k) * np.sum([1 / j for j in range(i, k + 1)])\n",
        "\n",
        "    # Normalize the squared singular values for comparison with the broken stick values\n",
        "    singular_values_squared_n = singular_values**2 / np.sum(singular_values**2)\n",
        "    # Initialize Bokeh figure\n",
        "    p = figure(title=\"Broken Stick Model vs Singular Values\",\n",
        "               x_axis_label=\"Index\",\n",
        "               y_axis_label=\"Proportion of Variance\",\n",
        "               width=width, height=height)\n",
        "    p.title.text_font_size = '20pt'\n",
        "    p.xaxis.axis_label_text_font_size = '16pt'\n",
        "    p.yaxis.axis_label_text_font_size = '16pt'\n",
        "    p.xaxis.major_label_text_font_size = '12pt'\n",
        "    p.yaxis.major_label_text_font_size = '12pt'\n",
        "\n",
        "    # Plot the normalized singular values\n",
        "    indices = np.arange(1, k + 1)\n",
        "    p.line(indices, singular_values_squared_n, line_width=2, color=\"blue\", legend_label=\"Singular Values\")\n",
        "    p.scatter(indices, singular_values_squared_n, size=8, color=\"blue\")\n",
        "\n",
        "    # Plot the normalized broken stick values\n",
        "    p.line(indices, broken_stick, line_width=2, line_dash=\"dashed\", color=\"red\", legend_label=\"Broken Stick\")\n",
        "    p.scatter(indices, broken_stick, size=8, color=\"red\")\n",
        "\n",
        "    # Customize legend and toolbar\n",
        "    p.legend.title = \"Legend\"\n",
        "    p.legend.location = \"top_right\"\n",
        "    p.legend.click_policy = \"hide\"\n",
        "    p.toolbar_location = \"below\"\n",
        "\n",
        "    # Determine the number of significant singular values using the broken stick rule\n",
        "    #SSVs = np.where(singular_values_normalized > broken_stick_normalized)[0][-1] + 1\n",
        "\n",
        "    SSVs = 0\n",
        "    for i in range(k):\n",
        "      if singular_values_squared_n[i]> broken_stick[i]:\n",
        "        SSVs += 1\n",
        "      else:\n",
        "        break\n",
        "\n",
        "    # Add a toggle button to control the legend visibility\n",
        "    button = Button(label=\"Toggle Legend\", button_type=\"success\")\n",
        "    button.js_on_click(CustomJS(args=dict(legend=p.legend), code=\"\"\"\n",
        "        legend.visible = !legend.visible;\n",
        "    \"\"\"))\n",
        "\n",
        "    #Show the plot with the toggle button\n",
        "    plot=column(p,button)\n",
        "    sol={'SSVs':SSVs, \"plot\":plot}\n",
        "    return sol\n",
        "\n",
        "\n",
        "\n",
        "# Function for matrix approximation from the number of SSV selected:\n",
        "def matrix_approximation(A, n):\n",
        "    \"\"\"\n",
        "    Approximates matrix A using the top n singular values.\n",
        "\n",
        "    Parameters:\n",
        "    - A: The original matrix to approximate.\n",
        "    - n: Number of significant singular values to use for approximation.\n",
        "\n",
        "    Returns:\n",
        "    - A_approx: The approximated matrix.\n",
        "    \"\"\"\n",
        "    # Perform SVD using scipy.linalg.svd\n",
        "    U, Sigma, VT = svd(A, full_matrices=False)\n",
        "\n",
        "    # Truncate the matrices to keep only the top 'n' singular values\n",
        "    U_n = U[:, :n]             # Keep the first 'n' columns of U\n",
        "    Sigma_n = np.diag(Sigma[:n])  # Keep the first 'n' singular values (diagonal matrix)\n",
        "    VT_n = VT[:n, :]           # Keep the first 'n' rows of V^T\n",
        "\n",
        "    # Compute the approximated matrix\n",
        "    A_approx = np.dot(U_n, np.dot(Sigma_n, VT_n))  # A_approx = U_n * Sigma_n * VT_n\n",
        "\n",
        "    return A_approx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Functions for the model\n",
        "\n",
        "\n",
        " # Name pending\n",
        "def kinetic_model_matrix(n_species, k_vals):\n",
        "    \"\"\"\n",
        "    Creates the ODE matrix to represent a system of species with the specified rate constants.\n",
        "\n",
        "    Parameters:\n",
        "        n_species (int): Number of species in the system.\n",
        "        params (dict): Dictionary of rate constants, e.g., {'k1': value, 'k_1': value, 'k2': value, ...}.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Matrix that aligns with the ODEs specified for each species.\n",
        "    \"\"\"\n",
        "    # Initialize an n_species x n_species matrix with zeros\n",
        "    ode_matrix = np.zeros((n_species, n_species))\n",
        "\n",
        "    # Populate the ODE matrix according to the specified rules\n",
        "    for i in range(n_species):\n",
        "        # Rate constant for reaction from species i to species i+1, if within bounds\n",
        "        if i + 1 < n_species:\n",
        "            ode_matrix[i, i] -= k_vals.get(f'k{i+1}', 0)      # Outflow from species i to i+1\n",
        "            ode_matrix[i + 1, i] += k_vals.get(f'k{i+1}', 0)  # Inflow to species i+1 from i\n",
        "\n",
        "        # Rate constant for reaction from species i+1 back to species i, if within bounds\n",
        "        if i - 1 >= 0:\n",
        "            ode_matrix[i, i] -= k_vals.get(f'k_{i}', 0)       # Outflow from species i to i-1\n",
        "            ode_matrix[i - 1, i] += k_vals.get(f'k_{i}', 0)   # Inflow to species i-1 from i\n",
        "\n",
        "    return ode_matrix\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def deriv_conc(conc,t, ks_matrix):\n",
        "  '''\n",
        "  Calculate the concentration derivative of every species .\n",
        "  The system of ODE's characterizing the reaction model is passed as a matrix with the rate constants\n",
        "  as coefficients. (REVISE)\n",
        "\n",
        "  Parameters\n",
        "      conc: Array con las concentraciones de las especies.\n",
        "      t : times points at which to calculate de derivative of the concentration with respect time.\n",
        "      params: a dictionary that contains the rate constants.\n",
        "\n",
        "  Returns:\n",
        "      np.ndarray: vector that contains the derivative of the concentrations of each species\n",
        "                  at the specified time point t. the rate of change of conecntration for each\n",
        "                  species in the system at the given time t.\n",
        "\n",
        "  '''\n",
        "  return np.dot(ks_matrix, conc)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def solv_conc_profile (k_vals, f_deriv, Conc_0, t):\n",
        "  \"\"\"\n",
        "  Solves the concentration profile of the reaction over time using\n",
        "  a 4th-order Runge-Kutta (RK4) method, allowing for variable time steps.\n",
        "\n",
        "  Parameters:\n",
        "  - f: Function that computes the derivative (reaction model)\n",
        "  - y0: Initial concentrations of the species\n",
        "  - t: Array or list of time points (can have non-uniform intervals)\n",
        "  - k_vals: Dictionary of reaction kinetic constants needed for the reaction model\n",
        "\n",
        "  Returns:\n",
        "  - df: DataFrame containing the cncentration profile for each species over time\n",
        "  \"\"\"\n",
        "\n",
        "  # Extract Conc_0 from 'initial_conc' in params\n",
        "  initial_conc = np.array(list(Conc_0.values()))\n",
        "\n",
        "\n",
        "  n_steps=len(t)\n",
        "  n_species = len(initial_conc)\n",
        "\n",
        "\n",
        "  # Initialize the solution array to store each species' concentration at each time step\n",
        "  solution = np.zeros((n_steps, n_species))\n",
        "  #print(solution[0])\n",
        "  solution[0]= initial_conc # Initial conditions\n",
        "\n",
        "  # We create the ODE system as matrix with the rate constants dispossed as its coefficients\n",
        "  MCoef= kinetic_model_matrix(n_species, k_vals)\n",
        "\n",
        "\n",
        "\n",
        "  # Iterate through each time step using the function deriv_RK from funcionesGenerales\n",
        "  for i in range(1, n_steps):\n",
        "    current_t = t[i-1]\n",
        "    next_t =t[i]\n",
        "    current_y = solution[i-1]\n",
        "\n",
        "    # Here, calculate the time intercal (delta_t) dynamically\n",
        "    delta_t = next_t - current_t\n",
        "\n",
        "\n",
        "    # Use deriv_RK to calculate the next step, passing `f_deriv` as the first argument\n",
        "    solution[i] = current_y + delta_t * deriv_RK(\n",
        "        f_deriv, current_y, current_t, delta_t, MCoef\n",
        "    )\n",
        "\n",
        "  # df = pd.DataFrame(solution, index=t, columns= [\"A\", \"B\", \"C\", \"D\"]) # shape (time, species)\n",
        "  # Generate column names based on the number of species\n",
        "  column_names = [f\"{chr(65 + j)}\" for j in range(n_species)]  # 'A', 'B', 'C', ...\n",
        "\n",
        "  # Create the DataFrame without empty columns\n",
        "  df = pd.DataFrame(solution, index=t, columns=column_names)  # shape (time, species)\n",
        "  return df\n",
        "\n",
        "#concentration_matrix_rk4 = solv_conc_profile(initial_ks, deriv_conc, initial_conc, t=df.index)\n",
        "\n",
        "# Output the concentration matrix\n",
        "#concentration_matrix_rk4\n",
        "\n",
        "#def solv_conc_profile (params, f_deriv, Conc_0, t):\n",
        "def species_spectra (k_vals, f_deriv, Conc_0, t, abs, pathlength, method, Lower_bound, min_value):\n",
        "  initial_conc = np.array(list(Conc_0.values()))\n",
        "\n",
        "\n",
        "\n",
        "  n_species = len(initial_conc)\n",
        "  # Extract the reaction model (fDeriv)\n",
        "  #model = k_vals.get('fDeriv')\n",
        "\n",
        "  C_profile = solv_conc_profile(k_vals, f_deriv, Conc_0,t)\n",
        "\n",
        " #*******\n",
        "  # Until here is the calculation of the spectra explicitly with assumptions\n",
        "\n",
        "\n",
        "  if method ==\"Explicit\":\n",
        "    # Generalized code to find max value and corresponding index\n",
        "    max_indices = {}\n",
        "    for col in C_profile.columns:\n",
        "      # max_value = C_profile[col].max()\n",
        "      max_index = C_profile[col].idxmax() # Get the index of the max value\n",
        "      max_indices [col] = max_index\n",
        "\n",
        "    # Use the indices found to slice the DataFrames\n",
        "    indices =list(max_indices.values())\n",
        "    # Calculate the concentration for the first and last species\n",
        "    first_species_c = C_profile.loc[indices[0]].iloc[0] # Maximum concentration of the first species\n",
        "    last_species_c = C_profile.loc[indices[-1]].iloc[-1] # Maximum concentration of the last species\n",
        "    # Calculate the spectra for the first and last species\n",
        "    first_species_s = (abs.loc[indices[0]]/(pathlength*first_species_c)).to_frame().T\n",
        "    last_species_s = (abs.loc[indices[-1]]/(pathlength*last_species_c)).to_frame().T\n",
        "    #print(last_species_s)\n",
        "\n",
        "    # Identify the reduced indices (excluding the first and last species)\n",
        "    red_indices = indices[1:-1]\n",
        "\n",
        "    if len(red_indices)>0:      # Ensure red_indices is not empty\n",
        "      # Extract the relevant concentration data for the reduced species\n",
        "      reduced_conc=C_profile.loc[red_indices].iloc[:,1:-1]\n",
        "      #print(C_profile.loc[red_indices].iloc[:,0].values)\n",
        "      #print(C_profile.loc[red_indices].iloc[:,-1])\n",
        "      reduced_abs =abs.loc[red_indices] - (C_profile.loc[red_indices].iloc[:,0].values*first_species_s.values) - (C_profile.loc[red_indices].iloc[:,-1].values*last_species_s.values)\n",
        "\n",
        "      # Solve the system of equations C^-1*A = E\n",
        "      s_red= pd.DataFrame(np.dot(np.linalg.inv(reduced_conc), reduced_abs), index=red_indices, columns=abs.columns)\n",
        "      #print(s_red)\n",
        "      sol_expl=pd.concat([first_species_s,s_red,  last_species_s])\n",
        "      #print(sol_expl)\n",
        "    else:\n",
        "      sol_expl= pd.concat([first_species_s, last_species_s])\n",
        "\n",
        "    # Assign alphabetical names to the indices (A, B, C, ...)\n",
        "    alphabet_indices = [chr(65 + i) for i in range(len(indices))]  # 65 is ASCII for 'A'\n",
        "    sol_expl.index =[alphabet_indices]\n",
        "    result=sol_expl\n",
        "\n",
        "  # Now we are going to use the implicit approach of the explicit approach above\n",
        "  elif method == \"Implicit\":\n",
        "     # Generalized code to find max value and corresponding index\n",
        "    max_indices = {}\n",
        "    for col in C_profile.columns:\n",
        "      # max_value = C_profile[col].max()\n",
        "      max_index = C_profile[col].idxmax() # Get the index of the max value\n",
        "      max_indices [col] = max_index\n",
        "\n",
        "    # Use the indices found to slice the DataFrames\n",
        "    indices =list(max_indices.values())\n",
        "    reduced_conc=C_profile.loc[indices]\n",
        "    reduced_abs =abs.loc[indices]\n",
        "    # Solve the system of equations C^-1*A = E\n",
        "    sol_imp = np.dot(np.linalg.inv(reduced_conc), reduced_abs)\n",
        "\n",
        "    # Assign alphabetical names to the indices (A, B, C, ...)\n",
        "    alphabet_indices = [chr(65 + i) for i in range(len(indices))]  # 65 is ASCII for 'A'\n",
        "    sol_imp=pd.DataFrame(sol_imp, index=alphabet_indices, columns=abs.columns)\n",
        "    sol_imp=sol_imp\n",
        "    result=sol_imp\n",
        "\n",
        "  elif method == \"Pseudo-inverse\":\n",
        "    # Now we are going to use the pseudoinverse of the concentration to estimate the spectroscopic species\n",
        "    # (extinction coefficients)\n",
        "    sol_ps=np.dot(np.linalg.pinv(C_profile),datos_approx_df)\n",
        "    alphabet_indices = [chr(65 + i) for i in range(len(C_profile.columns))]  # 65 is ASCII for 'A'\n",
        "    sol_ps=pd.DataFrame(sol_ps, index=alphabet_indices, columns=abs.columns)\n",
        "    sol_ps=sol_ps\n",
        "    result=sol_ps\n",
        "  else:\n",
        "    print(\"Input for method not valid\")\n",
        "\n",
        "  if Lower_bound:\n",
        "    result = result.clip(lower=min_value)\n",
        "  else:\n",
        "    result = result\n",
        "\n",
        "  #return sol, sol_imp, sol_ps\n",
        "  return result\n",
        "#def species_spectra (params, f_deriv, Conc_0, t, abs, pathlength):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Model_spectra(k_vals,f_deriv, Conc_0, t, abs, pathlength, original_data, method,Lower_bound, min_value, fitting = True):\n",
        "\n",
        "  n_species = len(np.array(list(Conc_0.values())))\n",
        "  #Conc_0= Conc_0 # with the \"*\" operator we unpack the values of the dictionary\n",
        "\n",
        "  #Solve for concentrations\n",
        "  C_matrix=solv_conc_profile(k_vals, f_deriv, Conc_0,t)\n",
        "  #print(f'C_matrix{type(C_matrix)}')\n",
        "\n",
        "  #Construct full extinction coefficient matrix\n",
        "  S_matrix= species_spectra (k_vals, f_deriv, Conc_0, t, abs, pathlength, method,Lower_bound, min_value)\n",
        "  #print(f'S_matrix{type(S_matrix)}')\n",
        "  #print(\"Shape of C_matrix:\", C_matrix.shape)  # Concentration matrix\n",
        "  #print(\"Shape of S_matrix:\", S_matrix.shape)  # Extinction coefficient matrix\n",
        "\n",
        "  #Use Lambert-Beer Law to calculate predicted absorbance (n_Lambda x n_t)\n",
        "  D_model = pathlength * np.dot(C_matrix, S_matrix)\n",
        "\n",
        "  D_exp = abs\n",
        "  D_org = original_data\n",
        "  #print(\"Shape of D_exp:\", D_exp.shape)  # Experimental absorbance\n",
        "  #print(\"Shape of D_model:\", D_model.shape)  # Predicted absorbance\n",
        "\n",
        "  #Compute residuals (difference between experimental abd predicted absorbance)\n",
        "  residuals_denoised= D_exp - D_model\n",
        "  residuals= D_org - D_model\n",
        "  #print(f'residuals{type(residuals)}')\n",
        "  D_model_df =pd.DataFrame(D_model, index=D_exp.index, columns= D_exp.columns)\n",
        "\n",
        "  if fitting:\n",
        "    sol= D_model_df.values.flatten()\n",
        "  else:\n",
        "    sol = {\n",
        "    \"params\": k_vals,\n",
        "    \"Conc_0\": Conc_0,\n",
        "    \"pathlength\": pathlength,\n",
        "    \"n_species\": n_species,\n",
        "    \"D_orig\": original_data,\n",
        "    \"D_approx\": abs,\n",
        "    \"D_model\": D_model_df,\n",
        "    \"C_matrix\": C_matrix,\n",
        "    \"S_matrix\": S_matrix,\n",
        "    \"residuals\": residuals,\n",
        "    \"residuals_denoised\": residuals_denoised\n",
        "}\n",
        "  #return D_model_df\n",
        "  return sol\n",
        "\n",
        "def create_dynamic_plot(df1, df2, Title, x_axis, y_axis, Legend, df1_label, df2_label, width=1200, height=700):\n",
        "    # Create a figure\n",
        "    p = figure(title=Title,\n",
        "               x_axis_label=x_axis,\n",
        "               y_axis_label=y_axis,\n",
        "               width=width, height=height)\n",
        "\n",
        "    # Define font sizes for the title, axes, and labels\n",
        "    p.title.text_font_size = '20pt'\n",
        "    p.xaxis.axis_label_text_font_size = '16pt'\n",
        "    p.yaxis.axis_label_text_font_size = '16pt'\n",
        "    p.xaxis.major_label_text_font_size = '12pt'\n",
        "    p.yaxis.major_label_text_font_size = '12pt'\n",
        "\n",
        "    # Check if the inputted columns are valid\n",
        "    col_names = df1.columns\n",
        "    n_lines = len(col_names)  # Total number of series\n",
        "\n",
        "    # Generate two distinct colors for each DataFrame\n",
        "    df1_color = \"#1f77b4\"  # A blue color for df1\n",
        "    df2_color = \"#ff7f0e\"  # An orange color for df2\n",
        "\n",
        "    indices = pd.to_numeric(df1.index)\n",
        "\n",
        "    # Create a list to store the line objects (to toggle their visibility)\n",
        "    lines = []\n",
        "\n",
        "    # Plot all the lines (one for each series in df1 and df2)\n",
        "    for i, col_name in enumerate(col_names):\n",
        "        # Ensure col_name is a string for the legend label\n",
        "        col_name_str = str(col_name)\n",
        "\n",
        "        # For df1, use the same color for all lines\n",
        "        line1 = p.line(indices, df1[col_name], legend_label=col_name_str,\n",
        "                       line_width=2, color=df1_color, line_dash=\"solid\", name=f\"line1_{i}\")\n",
        "        # For df2, use the same color for all lines\n",
        "        line2 = p.line(indices, df2[col_name], legend_label=col_name_str,\n",
        "                       line_width=2, color=df2_color, line_dash=\"dashed\", name=f\"line2_{i}\")\n",
        "        lines.append((line1, line2))\n",
        "\n",
        "    # Customize the legend\n",
        "    p.legend.title = Legend\n",
        "    p.legend.location = \"top_right\"\n",
        "    p.legend.click_policy = \"hide\"  # Allows hiding lines by clicking their labels\n",
        "    p.toolbar_location = \"below\"\n",
        "    p.legend.label_text_font_size = '12pt'\n",
        "    p.legend.title_text_font_size = '14pt'\n",
        "\n",
        "    # Create a dropdown menu for selecting series using df1 column names\n",
        "    series_select = Select(title=\"Select Series\", value=str(0), options=[(str(i), str(col_name)) for i, col_name in enumerate(col_names)])\n",
        "\n",
        "    # JavaScript callback for updating the visibility of the lines based on selection\n",
        "    callback = CustomJS(args=dict(lines=lines), code=\"\"\"\n",
        "        var selected_index = parseInt(cb_obj.value);\n",
        "        for (var i = 0; i < lines.length; i++) {\n",
        "            lines[i][0].visible = (i == selected_index);  // Show the selected series (df1)\n",
        "            lines[i][1].visible = (i == selected_index);  // Show the corresponding model (df2)\n",
        "        }\n",
        "    \"\"\")\n",
        "\n",
        "    # Attach the callback to the dropdown menu\n",
        "    series_select.js_on_change('value', callback)\n",
        "\n",
        "    # Initially, set the visibility for the first series\n",
        "    for i, (line1, line2) in enumerate(lines):\n",
        "        if i == 0:\n",
        "            line1.visible = True\n",
        "            line2.visible = True\n",
        "        else:\n",
        "            line1.visible = False\n",
        "            line2.visible = False\n",
        "\n",
        "    # Create a button to toggle the visibility of the legend\n",
        "    toggle_legend_button = Button(label=\"Toggle Legend\", button_type=\"success\")\n",
        "\n",
        "    # JavaScript callback to toggle the visibility of the legend\n",
        "    toggle_legend_callback = CustomJS(args=dict(legend=p.legend[0]), code=\"\"\"\n",
        "        legend.visible = !legend.visible;  // Toggle the visibility of the legend\n",
        "    \"\"\")\n",
        "\n",
        "    # Attach the callback to the button\n",
        "    toggle_legend_button.js_on_click(toggle_legend_callback)\n",
        "\n",
        "    # Initial visibility legend\n",
        "    p.legend.visible = False\n",
        "\n",
        "    # Return the plot, series select dropdown, and button in a layout\n",
        "    return column(p, series_select, toggle_legend_button)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTxq757Yinnq",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Upload files\n",
        "\n",
        "#@markdown Here the user has the posibility to upload different combinations of files as mentioned in the README.\n",
        "\n",
        "\n",
        "# Upload the file and save in a dictionary\n",
        "uploaded=files.upload()\n",
        "\n",
        "# Obtain the uploaded file name from the dictionary\n",
        "\n",
        "\n",
        "file_names = list(uploaded.keys())\n",
        "\n",
        "\n",
        "\n",
        "datos_org, file_name= lee_espectro(file_names)\n",
        "#datos = pd.read_csv(file_name,skiprows=[0])\n",
        "\n",
        "\n",
        "datos=datos_org"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Slicing Dataset\n",
        "# @markdown Here the user can slice the dataset.\n",
        "# @markdown\n",
        "# @markdown\n",
        "\n",
        "# @markdown Check the box below to perform dataset slicing\n",
        "Slicing = False  # @param {type: \"boolean\"}\n",
        "\n",
        "\n",
        "# @markdown Here the user specifies the range of time-points to keep (example: 0.001 to 0.02)\n",
        "t_start = None #@param {type:\"raw\"}\n",
        "\n",
        "t_end = None #@param {type:\"raw\"}\n",
        "#@markdown ****\n",
        "\n",
        "# @markdown Here the user specifies the range of wavelengths to keep (example: 400 to 600)\n",
        "wave_start = None #@param {type:\"raw\"}\n",
        "\n",
        "wave_end = None #@param {type:\"raw\"}\n",
        "\n",
        "# @markdown When inputting wavelengths, the exact number is not required. The function will find the closest wavelength to the entered value. For example, 400 will correspond to 398.820 instead of 402.140\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if Slicing:\n",
        "  datos= slice_dataset(datos, t_start, t_end, wave_start, wave_end)\n",
        "else:\n",
        "  print(\"No slicing event was performed\")\n",
        "\n",
        "datos"
      ],
      "metadata": {
        "id": "YNIwr_hmAJlw",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jFJONcZ2L1rN",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Spectra plot\n",
        "# @markdown Plots Absorbance vs Wavelength and Absorbance vs Time.\n",
        "\n",
        "df=datos\n",
        "df_transposed = df.T # Rows to columns and columns to rows\n",
        "\n",
        "wavelength_plot_2D = create_plot(df_transposed, Title = f\"Absorbance vs Wavelength // {file_name}\",\n",
        "                                         x_axis=\"Wavelength (nm)\", y_axis =\"Absorbance\",\n",
        "                                         Legend = \"Time (s)\")\n",
        "\n",
        "time_plot_2D = create_plot(df, Title= f\"Absorbance vs Time // {file_name}\",\n",
        "                                   x_axis = \"Time (s)\", y_axis = \"Absorbance\", Legend = \"Wavelength (nm)\" )\n",
        "\n",
        "plots = [wavelength_plot_2D,time_plot_2D]\n",
        "\n",
        "\n",
        "# Create tabs to display both plots\n",
        "wavelength_panel = TabPanel(child=wavelength_plot_2D, title=\"Wavelength Plot\")\n",
        "time_panel = TabPanel(child=time_plot_2D, title=\"Time Plot\")\n",
        "\n",
        "tabs = Tabs(tabs=[wavelength_panel, time_panel])\n",
        "\n",
        "# Show the plots\n",
        "show(tabs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1zcoW3sh-ek",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Singular Value Decomposition (SVD) and Identification of the Significant Singular Values (SSV)\n",
        "# Now we need a method to determine the number of significant singular values\n",
        "\n",
        "#@markdown **SVD Calculation**\n",
        "#@markdown\n",
        "#@markdown Only check the box below if you want to visualize the Singular Values (Sigma),\n",
        "#@markdown the transpose matrix (U) and the right matrix (VT).\n",
        "Check_SVD = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ****\n",
        "#@markdown **SSV Determination**\n",
        "scree_plot_th = 0.9 #@param{type: \"number\"}\n",
        "\n",
        "entropy_threshold= 0.9 #@param {type: \"number\"}\n",
        "if entropy_threshold <0 or entropy_threshold>1:\n",
        "  entropy_threshold=0.85\n",
        "  print(\"Value inputted not valid, entropy_threshold has taken the preset value of 0.85\")\n",
        "\n",
        "\n",
        "# Convert the dataframe into a Numpy array for the svd to work\n",
        "datos_array= datos.to_numpy()\n",
        "datos_array\n",
        "# Save the columns and row names\n",
        "Times= datos.index\n",
        "Wavelengths=datos.columns\n",
        "\n",
        "\n",
        "\n",
        "# Performn SVD\n",
        "U, Sigma, Vt = svd(datos_array, full_matrices= False)\n",
        "\n",
        "\n",
        "# Display results\n",
        "\n",
        "U_df = pd.DataFrame(U)\n",
        "\n",
        "Sigma_df = pd.DataFrame(Sigma)\n",
        "\n",
        "Vt_df = pd.DataFrame(Vt)\n",
        "if Check_SVD:\n",
        "  print(\"U Matrix:\\n\",U_df)\n",
        "  print(\"Singular Values:\\n\",Sigma_df)\n",
        "  print(\"V Transpose Matrix:\\n\",Vt_df)\n",
        "else:\n",
        "  pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Determine the number of significant singular values using the scree plot method\n",
        "n_significant_scree_plot = scree_plot_with_fit(Sigma, scree_plot_th)\n",
        "print(f\"Number of significant singular values (Scree Plot Method): {n_significant_scree_plot['SSVs']}\")\n",
        "\n",
        "# Determine the number of significant singular values using entropy-based selection\n",
        "n_significant_entropy = entropy_selection(Sigma, entropy_threshold)\n",
        "print(f\"\\nNumber of significant singular values (Entropy Method): {n_significant_entropy}\")\n",
        "\n",
        "# Determine the number of significant singular values using the broken stick method\n",
        "n_significant_broken_stick = broken_stick_method(Sigma)\n",
        "print(f\"\\nNumber of significant singular values (Broken Stick Method): {n_significant_broken_stick['SSVs']} \\n\")\n",
        "\n",
        "\n",
        "plots = [n_significant_scree_plot['plot'], n_significant_broken_stick['plot']]\n",
        "\n",
        "\n",
        "# Create tabs to display both plots\n",
        "scree_plot_panel = TabPanel(child=n_significant_scree_plot['plot'], title=\"Scree Plot\")\n",
        "broken_stick_panel = TabPanel(child=n_significant_broken_stick['plot'], title=\"Broken Stick Plot\")\n",
        "\n",
        "tabs = Tabs(tabs=[scree_plot_panel, broken_stick_panel])\n",
        "\n",
        "# Show the plots\n",
        "show(tabs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4E_Muw2GYkqM"
      },
      "outputs": [],
      "source": [
        "#@title Dimensionality reduction and Matrix Approximation\n",
        "#@markdown **Perform matrix approximation?**\n",
        "#@markdown\n",
        "#@markdown Uncheck the box below to NOT perform matrix approximation\n",
        "Answer = True  # @param {type: \"boolean\"}\n",
        "\n",
        "\n",
        "#@markdown ****\n",
        "\n",
        "#@markdown **Number of Significant Singular Values (SSVs):**\n",
        "SSVs = 3 #@param {type:\"number\"}\n",
        "#SSV = n_significant_entropy #@param [n_significant_entropy, n_significant_broken, n_significant_manual]\n",
        "\n",
        "\n",
        "if Answer:\n",
        "  datos_approx = matrix_approximation(datos_array, SSVs)\n",
        "  #print(datos_approx.shape)\n",
        "  #print(datos_array.shape)\n",
        "  print(f\"Approximation of the original data, using the first {SSVs} SSVs: \\n\")\n",
        "  datos_approx_df= pd.DataFrame(datos_approx, index=Times, columns= Wavelengths)\n",
        "  print(datos_approx_df.to_string())\n",
        "else:\n",
        "  datos_approx_df= datos\n",
        "  print(\"Matrix approximation was not performed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EErjJoVQ_uZf"
      },
      "outputs": [],
      "source": [
        "#@title Approximated Spectra plot\n",
        "# @markdown Plots Absorbance vs Wavelength and Absorbance vs Time.\n",
        "\n",
        "df=datos_approx_df\n",
        "df_transposed = df.T # Rows to columns and columns to rows\n",
        "\n",
        "wavelength_plot_2D = create_plot(df_transposed, Title = f\"Absorbance vs Wavelength // {file_name}\",\n",
        "                                         x_axis=\"Wavelength (nm)\", y_axis =\"Absorbance\",\n",
        "                                         Legend = \"Time (s)\")\n",
        "\n",
        "time_plot_2D = create_plot(df, Title= f\"Absorbance vs Time // {file_name}\",\n",
        "                                   x_axis = \"Time (s)\", y_axis = \"Absorbance\", Legend = \"Wavelength (nm)\" )\n",
        "\n",
        "plots = [wavelength_plot_2D,time_plot_2D]\n",
        "\n",
        "\n",
        "# Create tabs to display both plots\n",
        "wavelength_panel = TabPanel(child=wavelength_plot_2D, title=\"Wavelength Plot\")\n",
        "time_panel = TabPanel(child=time_plot_2D, title=\"Time Plot\")\n",
        "\n",
        "tabs = Tabs(tabs=[wavelength_panel, time_panel])\n",
        "\n",
        "# Show the plots\n",
        "show(tabs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81m0Wl1HXdO8",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Reaction Model Parameters\n",
        "\n",
        "#@markdown **Number of species:**\n",
        "n_species = 3 #@param{type: \"slider\", min:2, max:4, step:1}\n",
        "\n",
        "#@markdown **Pathlength of the cuvette (cm):**\n",
        "pathlength = 1 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown ****\n",
        "#@markdown **Lower bound for the spectroscopic species:**\n",
        "Lower_bound= False  # @param {type: \"boolean\"}\n",
        "min_value = 0 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown ****\n",
        "# Initial concentrations of species\n",
        "#@markdown **Initial Concentrations (μM):**\n",
        "A0 = 0 #@param {type:\"number\"}\n",
        "\n",
        "B0 = 0 #@param {type:\"number\"}\n",
        "\n",
        "C0 = 0 #@param {type:\"number\"}\n",
        "\n",
        "D0 = 0 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown ****\n",
        "\n",
        "# Enter the rate constants for the reactions\n",
        "#@markdown **Kinetic Rates (1/s):**\n",
        "k1 = 0 #@param {type:\"number\"}\n",
        "k1_fixed = True  # @param {type: \"boolean\"}\n",
        "#@markdown\n",
        "k_1 = 0 #@param {type:\"number\"}\n",
        "k_1_fixed = True  # @param {type: \"boolean\"}\n",
        "#@markdown\n",
        "k2 = 0 #@param {type:\"number\"}\n",
        "k2_fixed = True  # @param {type: \"boolean\"}\n",
        "#@markdown\n",
        "k_2 = 0 #@param {type:\"number\"}\n",
        "k_2_fixed = True  # @param {type: \"boolean\"}\n",
        "#@markdown\n",
        "k3 = 0 #@param {type:\"number\"}\n",
        "k3_fixed = True  # @param {type: \"boolean\"}\n",
        "#@markdown\n",
        "k_3 = 0 #@param {type:\"number\"}\n",
        "k_3_fixed = True  # @param {type: \"boolean\"}\n",
        "\n",
        "#@markdown ****\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define rate constants and their fixed status in a dictionary\n",
        "rate_constants_data = {\n",
        "    'k1': (k1, k1_fixed),\n",
        "    'k_1': (k_1, k_1_fixed),\n",
        "    'k2': (k2, k2_fixed),\n",
        "    'k_2': (k_2, k_2_fixed),\n",
        "    'k3': (k3, k3_fixed),\n",
        "    'k_3': (k_3, k_3_fixed)\n",
        "}\n",
        "\n",
        "# Initialize dictionaries for fixed and variable rate constants (these last ones are to be optimized later)\n",
        "fixed_ks = {}\n",
        "variable_ks = {}\n",
        "\n",
        "# Loop over each rate constant and classify it into fixed or variable\n",
        "for rate, (value, is_fixed) in rate_constants_data.items():\n",
        "    if is_fixed:\n",
        "        fixed_ks[rate] = value\n",
        "    else:\n",
        "        variable_ks[rate] = value\n",
        "\n",
        "# Output for debugging or checking\n",
        "#print(\"Fixed Rate Constants:\", fixed_ks)\n",
        "#print(\"Variable Rate Constants:\", variable_ks)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define initial concentrations and their fixed status in a dictionary\n",
        "concentration_data= {\n",
        "    'A0': A0,\n",
        "    'B0': B0,\n",
        "    'C0': C0,\n",
        "    'D0': D0\n",
        "}\n",
        "\n",
        "# Slice the dictionary using a subset of its keys\n",
        "species_list = ['A0', 'B0', 'C0', 'D0'][:n_species]\n",
        "# Here we get the sliced dictionary\n",
        "initial_conc = {key:concentration_data[key] for key in species_list}\n",
        "\n",
        "#\n",
        "\n",
        "# Output for debugging or checking\n",
        "#print(\"Fixed Concentrations:\", fixed_conc)\n",
        "#print(\"Variable Concentrations:\", variable_conc)\n",
        "\n",
        "\n",
        "# Output of the fixed and variable rate constants\n",
        "print(\"Fixed Rate Constants:\")\n",
        "for key, value in fixed_ks.items():\n",
        "    print(f\"{key} = {value}\")\n",
        "\n",
        "print(\"\\nVariable Rate Constants:\")\n",
        "for key, value in variable_ks.items():\n",
        "    print(f\"{key} = {value}\")\n",
        "\n",
        "print(\"\\nInitial Concentrations:\")\n",
        "for key, value in initial_conc.items():\n",
        "    print(f\"{key} = {value}\")\n",
        "\n",
        "# Group both rate constant dictionaries into one\n",
        "initial_ks = {**fixed_ks, **variable_ks}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3cByiA-Hzur",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Procesa\n",
        "\n",
        "#@markdown Method use for estimating the spectroscopic species:\n",
        "Method=\"Pseudo-inverse\" #@param[\"Pseudo-inverse\", \"Explicit\", \"Implicit\"]\n",
        "\n",
        "#@markdown The parameter above heavily influences the goodness of fitting.\n",
        "#@markdown Select **\"Pseudo-inverse\"** for the best fitting, however, it requires a reasonable\n",
        "#@markdown first estimation of the rate constant. Otherwise, select **\"Explicit\"** to obtain an initial\n",
        "#@markdown idea of the possible magnitude of the rate constants.\n",
        "initial_params= {**initial_ks }\n",
        "initial_params_var= {**variable_ks}\n",
        "\n",
        "\n",
        "#initial_params['n_species']=n_species # This dictionary contains the rate constants and the number of species\n",
        "#initial_params['pathlength']= pathlength # We add pathlength to the dictionary to streamline the parameters onto\n",
        "# a single dictionary\n",
        "\n",
        "# Prepare the list of the parameters names to be opimized\n",
        "nombrParVar = list(initial_params_var.keys())\n",
        "\n",
        "# Find the minimum value in the entire DataFrame\n",
        "#min_value = datos_approx_df.min().min()\n",
        "\n",
        "#cotaInf = [[min_value for val in row] for row in S_matrix]\n",
        "\n",
        "# Independent values (time and wavelength) and dependent values (Absorbance)\n",
        "fKwargs = dict(t=datos_approx_df.index.values,\n",
        "               f_deriv=deriv_conc,\n",
        "               Conc_0=initial_conc,\n",
        "               abs=datos_approx_df,\n",
        "               pathlength=pathlength,\n",
        "               original_data= datos,\n",
        "               method= Method,\n",
        "               Lower_bound=Lower_bound,\n",
        "               min_value=min_value,\n",
        "               fitting=True\n",
        "                )\n",
        "\n",
        "abs= datos_approx_df\n",
        "\n",
        "\n",
        "\n",
        "sol=procesa(argLeastSquares = argLeastSquares,\n",
        "            dictParEstim = initial_params,\n",
        "            nombrParVar = nombrParVar,\n",
        "            f = Model_spectra,\n",
        "            fKwargs = fKwargs,\n",
        "            Y = datos_approx_df.values.flatten()\n",
        "            #bounds=[cotaInf]\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ng98Lzll6Qkg",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Plots of Modelled data\n",
        "# @markdown This cell create plots with the information obtained by the model and\n",
        "#@markdown the optimization process.\n",
        "\n",
        "ad_parameters= sol['parAjustados']\n",
        "Model= Model_spectra( ad_parameters, deriv_conc, initial_conc,datos_approx_df.index, datos_approx_df, pathlength,datos,  Method, Lower_bound, min_value, fitting=False)\n",
        "df_e=Model['D_model']\n",
        "\n",
        "\n",
        "df_e_transposed = df_e.T # Rows to columns and columns to rows\n",
        "\n",
        "wavelength_plot_2D = create_plot(df_e_transposed, Title = f\"Absorbance vs Wavelength // {file_name}\",\n",
        "                                         x_axis=\"Wavelength (nm)\", y_axis =\"Absorbance\",\n",
        "                                         Legend = \"Time (s)\")\n",
        "\n",
        "time_plot_2D = create_plot(df_e, Title= f\"Absorbance vs Time // {file_name}\",\n",
        "                                   x_axis = \"Time (s)\", y_axis = \"Absorbance\", Legend = \"Wavelength (nm)\" )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_conc= Model['C_matrix']\n",
        "df_conc\n",
        "\n",
        "conc_profile_plot = create_plot(df_conc, Title = f\"Concentration over time // {file_name}\",\n",
        "                                         x_axis=\"Time (s)\", y_axis =\"Concentration (μM)\",\n",
        "                                         Legend = \"Species\")\n",
        "#show(conc_profile_plot)\n",
        "\n",
        "df_spectra=Model['S_matrix'].T\n",
        "df_spectra\n",
        "\n",
        "\n",
        "spectra_species_plot = create_plot(df_spectra, Title = f\"Spectroscopic species spectra // {file_name}\",\n",
        "                                         x_axis=\"Wavelength (nm)\", y_axis =\"Extinction Coefficient (1/(μM*cm))\",\n",
        "                                         Legend = \"Species\")\n",
        "#show(spectra_profile_plot)\n",
        "\n",
        "# Original - Model\n",
        "df_res=Model['residuals']\n",
        "df_res_transposed =df_res.T\n",
        "\n",
        "wavelength_res_plot_2D = create_plot(df_res_transposed, Title = f\"Absorbance vs Wavelength // {file_name}\",\n",
        "                                         x_axis=\"Wavelength (nm)\", y_axis =\"Absorbance\",\n",
        "                                         Legend = \"Time (s)\")\n",
        "time_res_plot_2D = create_plot(df_res, Title= f\"Absorbance vs Time // {file_name}\",\n",
        "                                   x_axis = \"Time (s)\", y_axis = \"Absorbance\", Legend = \"Wavelength (nm)\" )\n",
        "\n",
        "# Denoised - Model\n",
        "df_res_d=Model['residuals_denoised']\n",
        "df_res_d_transposed =df_res_d.T\n",
        "\n",
        "wavelength_res_d_plot_2D = create_plot(df_res_d_transposed, Title = f\"Absorbance vs Wavelength // {file_name}\",\n",
        "                                         x_axis=\"Wavelength (nm)\", y_axis =\"Absorbance\",\n",
        "                                         Legend = \"Time (s)\")\n",
        "time_res_d_plot_2D = create_plot(df_res_d, Title= f\"Absorbance vs Time // {file_name}\",\n",
        "                                   x_axis = \"Time (s)\", y_axis = \"Absorbance\", Legend = \"Wavelength (nm)\" )\n",
        "\n",
        "\n",
        "plots = [wavelength_plot_2D,wavelength_res_plot_2D,time_plot_2D,time_res_plot_2D, conc_profile_plot, spectra_species_plot,wavelength_res_d_plot_2D, time_res_d_plot_2D ]\n",
        "\n",
        "\n",
        "# Create tabs to display both plots\n",
        "wavelength_panel = TabPanel(child=wavelength_plot_2D, title=\"Wavelength Plot\")\n",
        "wavelength_res_panel = TabPanel(child=wavelength_res_plot_2D, title=\"Wavelength Residuals Plot\")\n",
        "wavelength_res_d_panel = TabPanel(child=wavelength_res_d_plot_2D, title=\"Wavelength Residuals Denoised Plot\")\n",
        "\n",
        "time_panel = TabPanel(child=time_plot_2D, title=\"Time Plot\")\n",
        "time_res_panel = TabPanel(child=time_res_plot_2D, title=\"Time Residuals Plot\")\n",
        "time_res_d_panel = TabPanel(child=time_res_d_plot_2D, title=\"Time Residuals Denoised Plot\")\n",
        "\n",
        "conc_profile_panel= TabPanel(child=conc_profile_plot, title=\"Concentration Profile\")\n",
        "spectra_species_spectra_panel = TabPanel(child= spectra_species_plot, title= \"Spectroscopic Species Spectra\")\n",
        "\n",
        "tabs = Tabs(tabs=[wavelength_panel, time_panel, conc_profile_panel, spectra_species_spectra_panel, wavelength_res_panel,wavelength_res_d_panel, time_res_panel, time_res_d_panel ])\n",
        "\n",
        "# Show the plots\n",
        "show(tabs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Modelled and Experimental data comparison\n",
        "#@markdown **Experimental data to use:**\n",
        "DF1_label=\"Original\" #@param[ \"Original\", \"Denoised\"]\n",
        "\n",
        "\n",
        "if DF1_label == \"Original\":\n",
        "  df1=datos\n",
        "  df1_transposed = df.T\n",
        "if DF1_label == \"Denoised\":\n",
        "  df1= datos_approx_df\n",
        "  df1_transposed = datos_approx_df.T\n",
        "\n",
        "# @markdown In the plots below the orange dashed line corresponds to the Modelled data and the blue solid line\n",
        "# @markdown corresponds to the Experimental data chosen above.\n",
        "\n",
        "\n",
        "\n",
        "df2=df_e #Model\n",
        "df2_transposed=df_e_transposed  #Model transposed\n",
        "\n",
        "\n",
        "# Generate plots\n",
        "wavelength_d_plot_2D = create_dynamic_plot(df1_transposed, df2_transposed,\n",
        "                                           Title=\"Absorbance vs Wavelength\",\n",
        "                                           x_axis=\"Wavelength (nm)\", y_axis=\"Absorbance\",\n",
        "                                           Legend=\"Time (s)\", df1_label=DF1_label, df2_label=\"Model\")\n",
        "\n",
        "time_d_plot_2D = create_dynamic_plot(df1, df2,\n",
        "                                     Title=\"Absorbance vs Time\",\n",
        "                                     x_axis=\"Time (s)\", y_axis=\"Absorbance\",\n",
        "                                     Legend=\"Wavelength (nm)\", df1_label=DF1_label, df2_label=\"Model\")\n",
        "\n",
        "# Create tabs to display both plots\n",
        "wavelength_d_panel = TabPanel(child=wavelength_d_plot_2D, title=\"Wavelength Plot\")\n",
        "time_d_panel = TabPanel(child=time_d_plot_2D, title=\"Time Plot\")\n",
        "\n",
        "tabs = Tabs(tabs=[wavelength_d_panel, time_d_panel])\n",
        "\n",
        "# Show the plots\n",
        "show(tabs, notebook_handle=True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "UY7dUddc3IXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import probplot\n",
        "\n",
        "\n",
        "\n",
        "#@title QQ plot of Modelled data\n",
        "#@markdown **Experimental data to use:**\n",
        "Residuals=\"Original_Modelled\" #@param[ \"Original_Modelled\", \"Denoised_Modelled\"]\n",
        "\n",
        "\n",
        "if Residuals == \"Original_Modelled\":\n",
        "  residuals=Model['residuals']\n",
        "  residuals_T=Model['residuals'].T\n",
        "  r_title= \"(Original - Modelled)\"\n",
        "if Residuals == \"Denoised_Modelled\":\n",
        "  residuals= Model['residuals_denoised']\n",
        "  residuals_T= Model['residuals_denoised'].T\n",
        "  r_title= \"(Denoised - Modelled)\"\n",
        "\n",
        "# @markdown In the plots below the red line is the theroretical distribution of the residuals, and the blue dots are the experiemental residuals.\n",
        "\n",
        "\n",
        "def create_dynamic_qqplot(df, Title, x_axis, y_axis, Legend, width=1200, height=700):\n",
        "    # Create a figure\n",
        "    p = figure(title=Title,\n",
        "               x_axis_label=x_axis,\n",
        "               y_axis_label=y_axis,\n",
        "               width=width, height=height)\n",
        "\n",
        "    # Define font sizes for the title, axes, and labels\n",
        "    p.title.text_font_size = '20pt'\n",
        "    p.xaxis.axis_label_text_font_size = '16pt'\n",
        "    p.yaxis.axis_label_text_font_size = '16pt'\n",
        "    p.xaxis.major_label_text_font_size = '12pt'\n",
        "    p.yaxis.major_label_text_font_size = '12pt'\n",
        "\n",
        "    # Check if the inputted columns are valid\n",
        "    col_names = df.columns\n",
        "\n",
        "    # Create a list to store scatter objects for toggling visibility\n",
        "    scatters = []\n",
        "    lines = []  # Store reference lines\n",
        "\n",
        "    # Loop through columns to create QQ plots\n",
        "    for i, col_name in enumerate(col_names):\n",
        "        # Extract the residuals for the current column\n",
        "        residuals = df[col_name].dropna()\n",
        "\n",
        "        # Generate QQ plot data (theoretical quantiles and residuals)\n",
        "        (qq_theoretical, qq_residuals), (slope, intercept, _) = probplot(residuals, dist=\"norm\")\n",
        "\n",
        "        # Add a scatter plot for the QQ plot\n",
        "        scatter = p.scatter(qq_theoretical, qq_residuals, legend_label=f\"QQ Plot: {col_name}\",\n",
        "                            size=8, color=\"#1f77b4\", name=f\"qq_scatter_{i}\")\n",
        "        scatters.append(scatter)\n",
        "\n",
        "        # Add the red theoretical line (y = slope * x + intercept)\n",
        "        x_line = [min(qq_theoretical), max(qq_theoretical)]\n",
        "        y_line = [slope * x + intercept for x in x_line]\n",
        "\n",
        "        line = p.line(x_line, y_line, line_width=2, color=\"red\", name=f\"qq_line_{i}\")\n",
        "        lines.append(line)\n",
        "\n",
        "    # Customize the legend\n",
        "    p.legend.title = Legend\n",
        "    p.legend.location = \"top_left\"\n",
        "    p.legend.click_policy = \"hide\"  # Allows hiding plots by clicking their labels\n",
        "    p.toolbar_location = \"below\"\n",
        "    p.legend.label_text_font_size = '12pt'\n",
        "    p.legend.title_text_font_size = '14pt'\n",
        "\n",
        "    # Create a dropdown menu for selecting QQ plots\n",
        "    series_select = Select(title=\"Select Series\", value=str(0),\n",
        "                           options=[(str(i), str(col_name)) for i, col_name in enumerate(col_names)])\n",
        "\n",
        "    # JavaScript callback for updating the visibility of the scatter and lines based on selection\n",
        "    callback = CustomJS(args=dict(scatters=scatters, lines=lines), code=\"\"\"\n",
        "        var selected_index = parseInt(cb_obj.value);\n",
        "        for (var i = 0; i < scatters.length; i++) {\n",
        "            scatters[i].visible = (i == selected_index);  // Show the selected QQ plot\n",
        "            lines[i].visible = (i == selected_index);     // Show the corresponding theoretical line\n",
        "        }\n",
        "    \"\"\")\n",
        "\n",
        "    # Attach the callback to the dropdown menu\n",
        "    series_select.js_on_change('value', callback)\n",
        "\n",
        "    # Initially, set the visibility for the first QQ plot\n",
        "    for i, (scatter, line) in enumerate(zip(scatters, lines)):\n",
        "        scatter.visible = (i == 0)\n",
        "        line.visible = (i == 0)\n",
        "\n",
        "    # Create a button to toggle the visibility of the legend\n",
        "    toggle_legend_button = Button(label=\"Toggle Legend\", button_type=\"success\")\n",
        "\n",
        "    # JavaScript callback to toggle the visibility of the legend\n",
        "    toggle_legend_callback = CustomJS(args=dict(legend=p.legend[0]), code=\"\"\"\n",
        "        legend.visible = !legend.visible;  // Toggle the visibility of the legend\n",
        "    \"\"\")\n",
        "\n",
        "    # Attach the callback to the button\n",
        "    toggle_legend_button.js_on_click(toggle_legend_callback)\n",
        "\n",
        "    # Return the plot, dropdown menu, and button in a layout\n",
        "    return column(p, series_select, toggle_legend_button)\n",
        "\n",
        "# Generate QQ plot\n",
        "qq_plot = create_dynamic_qqplot(residuals,\n",
        "                                Title= f'QQ Plot of Residuals of Absorbance vs Wavelength {r_title}',\n",
        "                                x_axis=\"Theoretical Quantiles\",\n",
        "                                y_axis=\"Residual Quantiles\",\n",
        "                                Legend=\"Residual Series\")\n",
        "\n",
        "wavelength_qq_plot = create_dynamic_qqplot(residuals_T,\n",
        "                                Title= f'QQ Plot of Residuals of Absorbance vs Wavelength {r_title}',\n",
        "                                x_axis=\"Theoretical Quantiles\",\n",
        "                                y_axis=\"Residual Quantiles\",\n",
        "                                Legend=\"Residual Series\")\n",
        "time_qq_plot = create_dynamic_qqplot(residuals,\n",
        "                                Title= f'QQ Plot of Residuals of Absorbance vs Time {r_title}',\n",
        "                                x_axis=\"Theoretical Quantiles\",\n",
        "                                y_axis=\"Residual Quantiles\",\n",
        "                                Legend=\"Residual Series\")\n",
        "\n",
        "\n",
        "# Create tabs to display both plots\n",
        "wavelength_qq_plot_panel = TabPanel(child=wavelength_qq_plot, title=\"Wavelength QQ Plot\")\n",
        "time_qq_plot_panel = TabPanel(child=time_qq_plot, title=\"Time QQ Plot\")\n",
        "\n",
        "tabs = Tabs(tabs=[wavelength_qq_plot_panel, time_qq_plot_panel])\n",
        "\n",
        "# Show the plots\n",
        "show(tabs, notebook_handle=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "sL2w6uN7dTgs",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmT3Zt6_TLLG",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Export results\n",
        "\n",
        "# @markdown Write the name for the zip file that contains the inputted and produced data.\n",
        "\n",
        "# Initial experimental data\n",
        "Model['D_orig'].to_csv('Original_experimental_data.csv', index=True)\n",
        "# Initial experimental data TRANSPOSE\n",
        "Model['D_orig'].T.to_csv('Original_experimental_data_TR.csv', index=True)\n",
        "\n",
        "# Denoised experimental data\n",
        "Model['D_approx'].to_csv('Denoised_experimental_data.csv', index=True)\n",
        "# Denoised experimental data TRANSPOSE\n",
        "Model['D_approx'].T.to_csv('Denoised_experimental_data_TR.csv', index=True)\n",
        "\n",
        "# Modeled data\n",
        "Model['D_model'].to_csv('Modelled_data.csv', index=True)\n",
        "# Modeled data TRANSPOSE\n",
        "Model['D_model'].T.to_csv('Modelled_data_TR.csv', index=True)\n",
        "\n",
        "# Residuals from Original - Modeled data\n",
        "Model['residuals'].to_csv('Residuals_OrigMod.csv', index=True)\n",
        "# Residuals from Original - Modeled data TRANSPOSE\n",
        "Model['residuals'].T.to_csv('Residuals_OrigMod_TR.csv', index=True)\n",
        "\n",
        "# Residuals from Denoised - Modeled data\n",
        "Model['residuals_denoised'].to_csv('Residuals_DenMod.csv', index=True)\n",
        "# Residuals from Denoised - Modeled data TRANSPOSE\n",
        "Model['residuals_denoised'].T.to_csv('Residuals_DenMod_TR.csv', index=True)\n",
        "\n",
        "# Concentration profile\n",
        "Model['C_matrix'].to_csv('Concentration_profile.csv', index=True)\n",
        "\n",
        "# Spectroscopic species\n",
        "Model['S_matrix'].to_csv('Spectroscopic_species.csv', index=True)\n",
        "\n",
        "\n",
        "\n",
        "# Create a CSV\n",
        "with open('Fitting_result.csv', mode='w', newline='') as file:\n",
        "  writer = csv.writer(file)\n",
        "\n",
        "  #Add blank rows\n",
        "  writer.writerow([''] * 7)\n",
        "  writer.writerow([''] * 7)\n",
        "\n",
        "  # n_species & pathlength Section\n",
        "  writer.writerow(['n_species', n_species])\n",
        "  writer.writerow(['pathlength (cm)', pathlength])\n",
        "  writer.writerow([''] * 7)\n",
        "\n",
        "  # Initial concentrations Section\n",
        "  writer.writerow(['INITIAL CONCENTRATIONS:'])\n",
        "  writer.writerow(['A0', Model['Conc_0']['A0']\n",
        "                   if 'AO' in Model['Conc_0'] else ''])\n",
        "  writer.writerow(['B0', Model['Conc_0']['B0']\n",
        "                   if 'BO' in Model['Conc_0'] else ''])\n",
        "  writer.writerow(['C0', Model['Conc_0']['C0']\n",
        "                   if 'CO' in Model['Conc_0'] else ''])\n",
        "  writer.writerow(['D0', Model['Conc_0']['D0']\n",
        "                   if 'DO' in Model['Conc_0'] else ''])\n",
        "\n",
        "  # Headers for the rate constants results\n",
        "  writer.writerow(['INITIAL ks', '', 'ADJUSTED ks','', 'STD ks'] )\n",
        "  #k1 row\n",
        "  writer.writerow(['k1', initial_ks['k1'], '', 'k1', sol['parAjustados']['k1'], '',\n",
        "        'k1_std', sol['sdPar'].get('k1_std', '') if 'k1' in variable_ks else ''])  # Display k1_std only if it is a variable parameter\n",
        "  #k_1 row\n",
        "  writer.writerow(['k_1', initial_ks['k_1'], '', 'k_1', sol['parAjustados']['k_1'], '',\n",
        "        'k_1_std', sol['sdPar'].get('k_1_std', '') if 'k_1' in variable_ks else ''])  # Display k_1_std only if it is a variable parameter\n",
        "\n",
        "  #k2 row\n",
        "  writer.writerow(['k2', initial_ks['k2'], '', 'k2', sol['parAjustados']['k2'], '',\n",
        "        'k2_std', sol['sdPar'].get('k2_std', '') if 'k2' in variable_ks else ''])  # Display k2_std only if it is a variable parameter\n",
        "  #k_2 row\n",
        "  writer.writerow(['k_2', initial_ks['k_2'], '', 'k_2', sol['parAjustados']['k_2'], '',\n",
        "        'k_2_std', sol['sdPar'].get('k_2_std', '') if 'k_2' in variable_ks else ''])  # Display k_2_std only if it is a variable parameter\n",
        "\n",
        "  #k3 row\n",
        "  writer.writerow(['k3', initial_ks['k3'], '', 'k3', sol['parAjustados']['k3'], '',\n",
        "        'k3_std', sol['sdPar'].get('k3_std', '') if 'k3' in variable_ks else ''])  # Display k3_std only if it is a variable parameter\n",
        "  #k_3 row\n",
        "  writer.writerow(['k_3', initial_ks['k_3'], '', 'k_3', sol['parAjustados']['k_3'], '',\n",
        "        'k_3_std', sol['sdPar'].get('k_3_std', '') if 'k_3' in variable_ks else ''])  # Display k_3_std only if it is a variable parameter\n",
        "\n",
        "\n",
        "  # Add blank rows\n",
        "  writer.writerow([''] * 7)\n",
        "  writer.writerow([''] * 7)\n",
        "\n",
        "  # R2 Section\n",
        "  writer.writerow(['R2'])\n",
        "  writer.writerow(['R2', sol['R2']])\n",
        "  writer.writerow([''] * 7)\n",
        "  writer.writerow([''] * 7)\n",
        "\n",
        "  # Detalles Section\n",
        "  writer.writerow(['Details'])\n",
        "\n",
        "  # x values (convert floats to strings to concatenate with 'x')\n",
        "  writer.writerow(['x'] + [str(x) for x in sol['detalles']['x']])\n",
        "\n",
        "  # cost\n",
        "  writer.writerow(['cost', sol['detalles']['cost']])\n",
        "\n",
        "  # Add blank rows\n",
        "  writer.writerow([''] * 7)\n",
        "\n",
        "  # grad values (convert floats to strings)\n",
        "  writer.writerow(['grad'] + [str(g) for g in sol['detalles']['grad']])\n",
        "\n",
        "  # optimality\n",
        "  writer.writerow(['optimality', sol['detalles']['optimality']])\n",
        "\n",
        "  # active_mask (convert integers to strings)\n",
        "  writer.writerow(['active_mask'] + [str(a) for a in sol['detalles']['active_mask']])\n",
        "\n",
        "  # nfev, njev, status, message, success\n",
        "  writer.writerow(['nfev', sol['detalles']['nfev']])\n",
        "  writer.writerow(['njev', sol['detalles']['njev']])\n",
        "  writer.writerow(['status', sol['detalles']['status']])\n",
        "  writer.writerow(['message', sol['detalles']['message']])\n",
        "  writer.writerow(['success', sol['detalles']['success']])\n",
        "\n",
        "  # fun values (split into multiple rows)\n",
        "  fun_values = sol['detalles']['fun']\n",
        "  for row in fun_values:\n",
        "    writer.writerow(['fun'] + [str(row)])\n",
        "\n",
        "  # Add blank rows\n",
        "  writer.writerow([''] * 7)\n",
        "\n",
        "  # jac values (split into multiple rows)\n",
        "  for row in sol['detalles']['jac']:\n",
        "      writer.writerow(['jac'] + [str(v) for v in row])\n",
        "\n",
        "  # Add blank rows\n",
        "  writer.writerow([''] * 7)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Then we proceed to save all the files and compressed them into a zip file\n",
        "\n",
        "# Take the current date and hour ()\n",
        "current_time = datetime.now().strftime(\"%d%m%Y%H%M%S\")\n",
        "\n",
        "# Define the prefix and create the complete name of the zip file\n",
        "name =\"spectra_\" #@param {type: \"string\"}\n",
        "zip_filename = f\"{name}{current_time}.zip\"\n",
        "\n",
        "# Create a zip file with the name written\n",
        "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "    # Add CSV files\n",
        "    zipf.write('Original_experimental_data.csv')\n",
        "    zipf.write('Original_experimental_data_TR.csv')\n",
        "\n",
        "    zipf.write('Denoised_experimental_data.csv')\n",
        "    zipf.write('Denoised_experimental_data_TR.csv')\n",
        "\n",
        "    zipf.write('Modelled_data.csv')\n",
        "    zipf.write('Modelled_data_TR.csv')\n",
        "\n",
        "    zipf.write ('Residuals_OrigMod.csv')\n",
        "    zipf.write ('Residuals_OrigMod_TR.csv')\n",
        "\n",
        "    zipf.write ('Residuals_DenMod.csv')\n",
        "    zipf.write ('Residuals_DenMod_TR.csv')\n",
        "    zipf.write ('Concentration_profile.csv')\n",
        "    zipf.write ('Spectroscopic_species.csv')\n",
        "    zipf.write ('Fitting_result.csv')\n",
        "\n",
        "\n",
        "\n",
        "# Download the zipped file\n",
        "files.download(zip_filename)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiltawqYiZRSofvlGXtvqC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}